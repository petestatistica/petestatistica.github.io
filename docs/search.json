[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PET Estatística",
    "section": "",
    "text": "Olá! Esta é a página do PET Estatística. Aqui você encontra informações sobre nosso grupo e sobre as atividades que desenvolvemos. Na seção Integrantes você encontra informações sobre os petianos e o tutor do grupo. Em Artigos, você encontrará textos sobre o universo da Estatística; os textos são produzidos por nós e toda semana tem artigo novo! Detalhes sobre o curso de Estatística da UFU estão disponíveis em O Curso. Por fim, a seção Sobre o PET contém informações sobre o Programa de Educação Tutorial.\n \n  \n   \n  \n    \n     email\n  \n  \n    \n     github\n  \n  \n    \n     instagram"
  },
  {
    "objectID": "artigos.html",
    "href": "artigos.html",
    "title": "Artigos",
    "section": "",
    "text": "A Blaze não manipula\n\n\n\n\n\n\nprobabilidade\n\n\n\nCom uma grande quantidade de apostadores e polêmicas, a Blaze precisa manipular seus jogos para ter lucro?\n\n\n\n\n\n14/02/2025\n\n\nJoel Machado Junior\n\n\n\n\n\n\n\n\n\n\n\n\nAumente suas chances de não ser roubado em Uberlândia\n\n\n\n\n\n\nsegurança\n\n\n\nEntender o comportamento de crimes é uma ferramenta muito importante para gerar ações capazes de evitá-los e também combatê-los\n\n\n\n\n\n05/08/2024\n\n\nBruno de Souza Melo\n\n\n\n\n\n\n\n\n\n\n\n\nPor trás do modelo KNN\n\n\n\n\n\n\nmodelos\n\n\nknn\n\n\n\nDescubra o poder do K-Nearest Neighbors (KNN), um algoritmo intuitivo para tarefas de classificação e regressão e que possui aplicações que vão desde a identificação de spam até diagnósticos médicos.\n\n\n\n\n\n19/07/2024\n\n\nRaiane Rosseti\n\n\n\n\n\n\n\n\n\n\n\n\nTeste de Hipóteses: Guia Prático para Análise Estatística\n\n\n\n\n\n\nteste de hipóteses\n\n\n\nUm guia prático para te ajudar a utilizar corretamente os testes de hipóteses.\n\n\n\n\n\n19/07/2024\n\n\nGabriel Moreira\n\n\n\n\n\n\n\n\n\n\n\n\nEstatística no Basquete: Transformando a Escolha do MVP\n\n\n\n\n\n\nesporte\n\n\nbasquete\n\n\n\nO uso da estatística no basquete: Nikola Jokic mereceu o prêmio MVP?\n\n\n\n\n\n12/07/2024\n\n\nGabriel Assis Godrim\n\n\n\n\n\n\n\n\n\n\n\n\nAs melhores recomendações\n\n\n\n\n\n\nmodelos de recomendação\n\n\n\nDescobrir sobre os tipos de modelos de recomendação e aplicações na nossa rotina.\n\n\n\n\n\n05/07/2024\n\n\nVictor Borin\n\n\n\n\n\n\n\n\n\n\n\n\nMapas de Calor\n\n\n\n\n\n\ngráficos\n\n\n\nExplicando o que são mapas de calor e como construí-los no R\n\n\n\n\n\n28/06/2024\n\n\nMaria Cecília Macedo\n\n\n\n\n\n\n\n\n\n\n\n\nUma medida regularmente esquecida\n\n\n\n\n\n\nmedidas de dispersão\n\n\nestatística\n\n\nfutebol\n\n\nanálise de desempenho\n\n\n\nA partir de exemplos práticos, este artigo mostra como o coeficiente de variação pode ser utilizado na análise de dados.\n\n\n\n\n\n21/06/2024\n\n\nPedro Garcia\n\n\n\n\n\n\nSem itens correspondentes"
  },
  {
    "objectID": "curso.html",
    "href": "curso.html",
    "title": "O Curso",
    "section": "",
    "text": "A tabela abaixo oferece um resumo do curso de Estatística da Universidade Federal de Uberlândia. Atualmente, o curso possui duas versões curriculares em vigor. Alunos que iniciaram antes do primeiro semestre de 2023 seguem a versão curricular de 2016, enquanto aqueles que ingressaram a partir desse semestre seguem a versão curricular de 2023.\n\n\n\n\n\n\n\nInformação\nDetalhes\n\n\n\n\nHabilitação\nBacharelado\n\n\nRegime acadêmico\nSemestral\n\n\nTurno de oferta\nNoturno\n\n\nDuração\nCurriculo 2016: 5 anos  Curriculo 2023: 4 anos\n\n\nPrazo mínimo\n4 anos\n\n\nPrazo máximo\nCurriculo 2016: 8 anos  Curriculo 2023: 6 anos\n\n\nNúmero de vagas\n30 por semestre\n\n\n\n\n\n\nEstatística é uma ciência multidisciplinar. Seu objetivo é o estudo da variabilidade, da incerteza e da tomada de decisões frente à incerteza. A variabilidade e a incerteza estão presentes em todas as áreas do conhecimento, o que torna a estatística uma ciência de importância crucial para resolver uma série de problemas, através do uso de metodologias que permitem chegar a conclusões científicas a partir de dados coletados do mundo real. O aprendizado a partir de dados, usando técnicas e metodologias científicas apropriadas e direcionadas às mais diversas aplicações, caracteriza a estatística como uma ciência multidisciplinar, embora seu corpo metodológico esteja inserido dentro das ciências exatas. Os métodos estatísticos têm forte embasamento matemático, mas o princípio que rege suas aplicações é o de quantificar a incerteza para fornecer conclusões científicas baseando-se em dados. Ferramentas estatísticas tais como coeficientes de confiança, níveis de significância e regiões de credibilidade foram desenvolvidas com a finalidade de fornecer resultados válidos e, relativamente, de fácil interpretação. O desenvolvimento da estatística como ciência tem seguido a tendência natural do mundo moderno. A alta competitividade na busca de tecnologias e de mercados passa obrigatoriamente pela necessidade da obtenção de informações e do rápido aprendizado das mesmas. A expansão no processo de obtenção, armazenamento e disseminação de informações estatísticas tem sido acompanhada pelo desenvolvimento de novas técnicas e metodologias.\n\n\n\nO estatístico é um profissional versátil que pode trabalhar em diversas áreas do conhecimento, interagindo com várias equipes multidisciplinares. A sólida formação em teoria e técnicas estatísticas, ciência de dados, probabilidade, computação e métodos quantitativos permite ao estatístico a construção de modelos, o controle e o estudo adequado de fenômenos que envolvam incerteza, além do desenvolvimento e aperfeiçoamento de novas técnicas estatísticas de obtenção e análise de informações.\n\n\n\nDe acordo com o Conselho Regional de Estatística da 3ª Região, há muitas vagas para estatísticos, ou para profissionais que dominam as técnicas estatísticas, no atual mercado de trabalho. Com a crescente áera de Ciência de Dados, a procura por nossos profissionais tem crescido ainda mais. Mas sofremos de um grave problema que é a falta de profissionais no mercado: a baixa procura pelos nossos cursos e a alta evasão escolar resultam em poucos bacharéis em estatística formados anualmente no Brasil."
  },
  {
    "objectID": "curso.html#dados-gerais",
    "href": "curso.html#dados-gerais",
    "title": "O Curso",
    "section": "",
    "text": "A tabela abaixo oferece um resumo do curso de Estatística da Universidade Federal de Uberlândia. Atualmente, o curso possui duas versões curriculares em vigor. Alunos que iniciaram antes do primeiro semestre de 2023 seguem a versão curricular de 2016, enquanto aqueles que ingressaram a partir desse semestre seguem a versão curricular de 2023.\n\n\n\n\n\n\n\nInformação\nDetalhes\n\n\n\n\nHabilitação\nBacharelado\n\n\nRegime acadêmico\nSemestral\n\n\nTurno de oferta\nNoturno\n\n\nDuração\nCurriculo 2016: 5 anos  Curriculo 2023: 4 anos\n\n\nPrazo mínimo\n4 anos\n\n\nPrazo máximo\nCurriculo 2016: 8 anos  Curriculo 2023: 6 anos\n\n\nNúmero de vagas\n30 por semestre"
  },
  {
    "objectID": "curso.html#o-que-é-estatística",
    "href": "curso.html#o-que-é-estatística",
    "title": "O Curso",
    "section": "",
    "text": "Estatística é uma ciência multidisciplinar. Seu objetivo é o estudo da variabilidade, da incerteza e da tomada de decisões frente à incerteza. A variabilidade e a incerteza estão presentes em todas as áreas do conhecimento, o que torna a estatística uma ciência de importância crucial para resolver uma série de problemas, através do uso de metodologias que permitem chegar a conclusões científicas a partir de dados coletados do mundo real. O aprendizado a partir de dados, usando técnicas e metodologias científicas apropriadas e direcionadas às mais diversas aplicações, caracteriza a estatística como uma ciência multidisciplinar, embora seu corpo metodológico esteja inserido dentro das ciências exatas. Os métodos estatísticos têm forte embasamento matemático, mas o princípio que rege suas aplicações é o de quantificar a incerteza para fornecer conclusões científicas baseando-se em dados. Ferramentas estatísticas tais como coeficientes de confiança, níveis de significância e regiões de credibilidade foram desenvolvidas com a finalidade de fornecer resultados válidos e, relativamente, de fácil interpretação. O desenvolvimento da estatística como ciência tem seguido a tendência natural do mundo moderno. A alta competitividade na busca de tecnologias e de mercados passa obrigatoriamente pela necessidade da obtenção de informações e do rápido aprendizado das mesmas. A expansão no processo de obtenção, armazenamento e disseminação de informações estatísticas tem sido acompanhada pelo desenvolvimento de novas técnicas e metodologias."
  },
  {
    "objectID": "curso.html#o-que-faz-o-estatístico",
    "href": "curso.html#o-que-faz-o-estatístico",
    "title": "O Curso",
    "section": "",
    "text": "O estatístico é um profissional versátil que pode trabalhar em diversas áreas do conhecimento, interagindo com várias equipes multidisciplinares. A sólida formação em teoria e técnicas estatísticas, ciência de dados, probabilidade, computação e métodos quantitativos permite ao estatístico a construção de modelos, o controle e o estudo adequado de fenômenos que envolvam incerteza, além do desenvolvimento e aperfeiçoamento de novas técnicas estatísticas de obtenção e análise de informações."
  },
  {
    "objectID": "curso.html#mercado-de-trabalho",
    "href": "curso.html#mercado-de-trabalho",
    "title": "O Curso",
    "section": "",
    "text": "De acordo com o Conselho Regional de Estatística da 3ª Região, há muitas vagas para estatísticos, ou para profissionais que dominam as técnicas estatísticas, no atual mercado de trabalho. Com a crescente áera de Ciência de Dados, a procura por nossos profissionais tem crescido ainda mais. Mas sofremos de um grave problema que é a falta de profissionais no mercado: a baixa procura pelos nossos cursos e a alta evasão escolar resultam em poucos bacharéis em estatística formados anualmente no Brasil."
  },
  {
    "objectID": "curso.html#histórico",
    "href": "curso.html#histórico",
    "title": "O Curso",
    "section": "Histórico",
    "text": "Histórico\nO curso de Estatística teve sua criação aprovada em 28 de abril de 2009 e a primeira turma ingressou no ano de 2010. O reconhecimento perante o MEC aconteceu 3 anos mais tarde, em 09 de julho de 2013. Durante este período, o curso formou diversos profissionais que hoje atuam em grandes empresas da nossa região e do país, além de vários ex-alunos que cursam ou que cursaram pós-graduação em centros educacionais de grande tradição na área. Desde a sua criação o curso recebe nota 4 nas avaliações periódicas da CAPES, sendo nota máxima igual a 5, se consolidando como um importante centro de formação. Ao longo dos anos, vários alunos foram beneficiados com diversas oportunidades concedidas pela Universidade e por órgão de apoio, como bolsas assistenciais, de iniciação científica, de monitoria, de intercâmbio e estágio remunerado."
  },
  {
    "objectID": "curso.html#objetivos-do-curso",
    "href": "curso.html#objetivos-do-curso",
    "title": "O Curso",
    "section": "Objetivos do Curso",
    "text": "Objetivos do Curso\nO Curso de graduação, Bacharelado em Estatística, tem por objetivo qualificar os seus graduados para ocupar posições de destaque no mercado de trabalho, interagindo com equipes multidisciplinares, junto a engenheiros, físicos, economistas, biólogos e outros profissionais, visando o desenvolvimento de pesquisa científica, quer dentro ou fora do ambiente acadêmico. Nesse contexto, o Curso de Bacharelado em Estatística tem o objetivo de formar profissionais com as seguintes características:\n\nsólida formação de conteúdos de Matemática, Métodos Quantitativos e Estatística;\nconhecimento de informática, especialmente em linguagens de programação, noções de banco de dados e pacotes estatísticos;\nformação que lhes prepare para enfrentar os desafios das rápidas transformações da sociedade, do mercado de trabalho e das condições do exercício profissional;\ndomínio de conhecimentos estatísticos, tendo consciência do modo de produção próprio dessa ciência – fundamentos, origens, procedimentos etc. – tendo, também, conhecimento das suas aplicações em várias áreas;\nconhecimento de conteúdos, habilidades e competências próprias da estatística, reconhecendo sua importância para o exercício pleno da profissão;\nresolução de problemas que envolvam a coleta, a sistematização e a análise de dados, frequentemente em colaboração com profissionais de outras áreas, que propicie uma grande variedade de ênfases possíveis, tais como: Probabilidade, Inferência, Bioestatística, Estatística Experimental, Qualidade e Confiabilidade, Ciência de Dados e Estatística computacional;\ncapacidade para continuidade dos estudos em cursos de pós-graduação, para atuação em universidades, centros de pesquisa e instituições similares, que enseje uma formação mais acadêmica e formal;\ncapazes de assumir postura ética diante dos fatos."
  },
  {
    "objectID": "curso.html#estrutura-curricular",
    "href": "curso.html#estrutura-curricular",
    "title": "O Curso",
    "section": "Estrutura Curricular",
    "text": "Estrutura Curricular\n\nVersão Curricular 2016\nNa versão de 2016 do Projeto Pedagógico do Curso (PPC), a grade curricular do curso de Graduação em Estatística é composta por 40 disciplinas, relacionados entre si através dos pré-requisitos e divididos em 10 períodos no horário noturno, sendo 4 disciplinas por semestre. Dessas disciplinas, 36 são de caráter obrigatório e 4 de caráter optativo, sendo esses escolhidos de um grupo de disciplinas oferecidos pelo Instituto de Matemática e Estatística (IME) e outras unidades acadêmicas da universidade. Além destes, o curso também possui a componente Horas Complementares. Nos links a seguir, você encontra informações detalhadas sobre o PPC, as fichas das disciplinas e a grade curricular da versão curricular de 2016.\n\n\n\nProjeto Pedagógico do Curso\nArquivo PDF\n\n\nFichas das disciplinas\nLink\n\n\nGrade Curricular\nArquivo PDF\n\n\n\n\n\nVersão Curricular 2023\nNa versão de 2023 do Projeto Pedagógico do Curso (PPC), a grade curricular passou a ser composta por 41 disciplinas divididos em 8 períodos. Dessas disciplinas, 38 são de caráter obrigatório e 3 de caráter optativo, sendo essas optativas escolhidos de um grupo de 29 disciplinas oferecidos pelo Instituto de Matemática e Estatística (IME) e outras unidades acadêmicas da universidade. Também existe a componente Horas Complementares e Atividades de Extensão. Não haverá migração de um currículo para outro, no entanto existem várias disciplinas que são equivalentes entre os dois currículos. Se você entrou no nosso curso a partir do segundo semestre de 2023 (que corresponde ao semestre letivo 2023.1) você está sob as regras do PPC 2023.1, caso contrário você está sob as regras do PPC 2016.1. Nos links a seguir, você encontra informações detalhadas sobre o PPC, a estrutura curricular e a grade curricular da versão curricular de 2023.\n\n\n\nProjeto Pedagógico do Curso\nArquivo PDF\n\n\nFichas das disciplinas\nLink\n\n\nGrade Curricular\nArquivo PDF"
  },
  {
    "objectID": "sobre-pet.html",
    "href": "sobre-pet.html",
    "title": "Sobre o PET",
    "section": "",
    "text": "O Programa de Educação Tutorial (PET) foi criado para apoiar atividades acadêmicas que integram ensino, pesquisa e extensão. A UFU é a instituição de ensino federal que contém o maior número de grupos PETs no Brasil. Existem dois tipos de Grupo PET na UFU, o PET MEC e o PET INSTITUCIONAL, este último criado, regido e controlado exclusivamente pela Universidade. O PET Estatística é um grupo institucional e foi criado em 2016.\nO PET é constituído por grupos de alunos organizados a partir das formações em nível de Graduação da UFU, cujas atividades são orientadas pelo princípio da indissociabilidade entre ensino, pesquisa e extensão. Sendo assim, o Programa é composto por Grupos tutoriais de aprendizagem e busca propiciar aos alunos, sob a orientação de um professor tutor, condições para a realização de atividades extracurriculares, que complementem a sua formação acadêmica, visando atender mais plenamente às necessidades do próprio Curso de Graduação, ampliando e aprofundando o percurso de sua formação profissional. Espera-se assim, contribuir para a melhoria da qualidade acadêmica dos Cursos de Graduação na UFU (Manual PET UFU, página 3, versão 2021)."
  },
  {
    "objectID": "sobre-pet.html#introdução",
    "href": "sobre-pet.html#introdução",
    "title": "Sobre o PET",
    "section": "",
    "text": "O Programa de Educação Tutorial (PET) foi criado para apoiar atividades acadêmicas que integram ensino, pesquisa e extensão. A UFU é a instituição de ensino federal que contém o maior número de grupos PETs no Brasil. Existem dois tipos de Grupo PET na UFU, o PET MEC e o PET INSTITUCIONAL, este último criado, regido e controlado exclusivamente pela Universidade. O PET Estatística é um grupo institucional e foi criado em 2016.\nO PET é constituído por grupos de alunos organizados a partir das formações em nível de Graduação da UFU, cujas atividades são orientadas pelo princípio da indissociabilidade entre ensino, pesquisa e extensão. Sendo assim, o Programa é composto por Grupos tutoriais de aprendizagem e busca propiciar aos alunos, sob a orientação de um professor tutor, condições para a realização de atividades extracurriculares, que complementem a sua formação acadêmica, visando atender mais plenamente às necessidades do próprio Curso de Graduação, ampliando e aprofundando o percurso de sua formação profissional. Espera-se assim, contribuir para a melhoria da qualidade acadêmica dos Cursos de Graduação na UFU (Manual PET UFU, página 3, versão 2021)."
  },
  {
    "objectID": "sobre-pet.html#objetivos-do-programa",
    "href": "sobre-pet.html#objetivos-do-programa",
    "title": "Sobre o PET",
    "section": "Objetivos do Programa",
    "text": "Objetivos do Programa\nO PET tem como objetivo promover a formação ampla e de qualidade dos alunos de graduação envolvidos direta ou indiretamente com o Programa, estimulando valores que reforcem a cidadania e a consciência social dos participantes e a melhoria dos Cursos de Graduação. São objetivos específicos do Programa:\n\nDesenvolver atividades acadêmicas de ensino, pesquisa e extensão, em padrões de qualidade e de excelência, mediante Grupos de aprendizagem tutorial de natureza coletiva e interdisciplinar;\nContribuir para a elevação da qualidade da formação dos alunos de graduação, a diminuição da evasão e a promoção do sucesso acadêmico, valorizando a articulação das atividades de ensino, pesquisa e extensão;\nEstimular a formação de profissionais e docentes de elevada qualificação técnica, científica, tecnológica e acadêmica;\nFormular novas estratégias de desenvolvimento e modernização do ensino superior no país;\nEstimular o espírito crítico, bem como a atuação profissional pautada pela cidadania e pela função social da educação superior;\nIntroduzir novas práticas pedagógicas na graduação;\nContribuir para a consolidação e difusão da educação tutorial como prática de formação na graduação;\nEstimular a vinculação dos Grupos a áreas prioritárias e a políticas públicas e de desenvolvimento, assim como a correção de desigualdades sociais;\nContribuir com a política de diversidade, por meio de ações afirmativas em defesa da equidade socioeconômica, étnico-racial e de gênero;\nContribuir com a política de saúde mental, buscando o bem-estar e a qualidade de vida da comunidade estudantil."
  },
  {
    "objectID": "sobre-pet.html#pet-estatística",
    "href": "sobre-pet.html#pet-estatística",
    "title": "Sobre o PET",
    "section": "PET Estatística",
    "text": "PET Estatística\nO PET Estatística é composto por um tutor e 12 alunos bolsistas, além de poder contar com a participação de até 4 alunos não bolsistas. Os alunos bolsistas recebem uma bolsa mensal no valor de R$ 700,00. O grupo se reúne duas vezes por semana para discutir o planejamento de seu trabalho e para desenvolver suas atividades internas, como o Clube do Livro sobre visualização de dados e o Grupo de Estudos em Processamento de Linguagem Natural. Além destas atividades, o PET Estatística desenvolve projetos de divulgação da Estatística e oferece regularmente cursos que contribuem para a formação dos alunos de toda a comunidade UFU."
  },
  {
    "objectID": "integrantes/index.html",
    "href": "integrantes/index.html",
    "title": "Integrantes",
    "section": "",
    "text": "Conheça um pouco mais sobre os integrantes da nossa equipe.\n\nPetianos\n\n\n\n\n\nBruno de Souza\n\nEntrei no curso em 2019 e estou no PET há quase 4 anos, cursando atualmente o último período da graduação. Um pseudo-cinéfilo com um gosto eclético (às vezes duvidoso) para músicas e camisas estampadas. Minha paixão é viajar e conhecer lugares e culturas diferentes.\n\n\n\n\nBruno Mesquita\n\nSou o Bruno, um eterno aprendiz que adora tecnologia, comida boa e me aventurar por esportes aleatórios! De Jandira-SP, me defino como um curioso nato e sempre pronto para um aventura. Ah, e não vivo sem cachorro quente com purê, uva e amendoim japonês!\n\n\n\n\nElisa\n\nEntrei na Estatística em 2020 e no PET em 2021. No meu tempo livre, gosto de ler, costurar e passar tempo com pessoas queridas. Sou de Campinas-SP e sinto muita saudade dos meus irmãos (tenho 4) e de purê no cachorro-quente.\n\n\n\n\nErik\n\nOlá, meu nome é Erik, tenho 21 anos e, apesar de ter dificuldade em me descrever, vou tentar. Gosto de Daft Punk, ficção científica e adoro conhecer novas pessoas.\n\n\n\n\nGabriel Assis\n\nTenho 21 anos e estou mais ou menos no sexto período da faculdade. No meu tempo livre, gosto muito de jogar basquete (mesmo não sendo bom), assistir filmes e séries, além de adorar assistir qualquer tipo de esporte, geralmente futebol e sofrendo com o Flamengo.\n\n\n\n\nGabriel Moreira\n\nGosto de castanha de caju, gatos e suco prats de laranja, não necessariamente nessa ordem. No meu tempo livre gosto de jogar, ler, fazer alguma atividade física, aprender coisas novas e refletir sobre o império romano.\n\n\n\n\nGiulia\n\nSou de Franca, interior de SP, sim, a capital do calçado, do café e, claro, do basquete! A cidade do maior do Brasil. A gente fala “uai”, “trem” e o “logo ali” é tão mineiro que você pode confundir. Somos mais mineiros que paulistas. Eu amo a Zaya Maria e o Tiramisù, e no meu tempo livre, sou expert em fazer nada. É uma arte!\n\n\n\n\nGladston\n\nSou Gladston, alguém que gosta de desafios intelectuais e tenta se aprimorar no que se propõe a fazer. Também gosto de xadrez, e o violino é minha maneira de expressar um pouco de sensibilidade.\n\n\n\n\nJoel\n\nSou de Vitória-ES. Descobri por acaso o curso de Estatística, em Uberlândia. Não tenho tatuagem, não tenho gato, não tenho cachorro e não tenho dinheiro. Tenho dificuldade em dizer do que gosto, além da matemática, literatura, xadrez e Iron Maiden.\n\n\n\n\nJônatas\n\nSou o Jônatas, de Araxá. Gosto de futebol, de ler, de contar algumas piadas (de qualidade bem questionável), e de aprender coisas novas. Sou muito curioso, e acho que esse é um dos fatores que me trouxe à Estatística.\n\n\n\n\nKalyara\n\nMe chamo Kalyara, sou apaixonada por flores, vermelho, pelos meus cachorros e por biscoito frito.\n\n\n\n\nMaria Cecília\n\nEntrei no PET no final de 2022. Não vivo sem meus fones de ouvido, amo animais marinhos, odeio ficar entediada e por isso possuo vários hobbies, alguns deles são jogar videogame, ler, tocar instrumentos e dançar sapateado.\n\n\n\n\nMoisés\n\nEscuto power metal, french house e sertanejo universitário. Jogo LoL, CS e futebol. Não torço para nenhum time, sou do tipo que assiste qualquer esporte que esteja passando. Acertei os últimos 3 campeões de Copa do Mundo. Tenho uma mente bastante viajada e me considero uma pessoa criativa. Sou o maior fã de Pepsi Black, de dormir escutando música triste e de ir ao cinema.\n\n\n\n\nPedro Garcia\n\nA vida me fez estatístico e faço da minha vida estatística, unindo-a aos meus hobbies. Desde uma análise dos detalhes de uma partida de futebol ou modalidade de eSports, até a criação de um recomendador tão bom que seja capaz de indicar Westworld, TWD e Interestelar.\n\n\n\n\nThiago\n\nMeu nome é Thiago e sou de Barbacena-MG. Já quis ser engenheiro, programador e físico até me encontrar na estatística. Torcedor fanático do Fluminense, apaixonado por futebol e fã do Messi. Também gosto de ouvir Rock/Metal e assistir filmes e séries de ficção científica.\n\n\n\n\nVictor\n\nMeu nome é Victor Borin, sou de Ribeirão Preto. Torço para o São Paulo e para o Comercial, amo jogar, aprender, ver qualquer tipo de esporte, além de amar quase todo tipo de música.\n\n\n\n\nTutor\n\n\n\n\n\n\nPedro Franklin\nQuando eu tinha 6 anos eu queria ser bailarino; 4 anos depois, pensei que seria goleiro do Cruzeiro; desisti do futebol aos 12 anos e estava convicto que seria zoólogo; fiquei perdido aos 24 anos e já não sabia o que queria ser. Atualmente vivo com dois gatinhos (Frida e Amendoim) e sou um aprendiz ceramista e jogador mediano de Zelda."
  },
  {
    "objectID": "artigos/mapas-calor/index.html",
    "href": "artigos/mapas-calor/index.html",
    "title": "Mapas de Calor",
    "section": "",
    "text": "Hoje irei apresentar um recurso gráfico que acho muito interessante na visualização de dados, o Mapa de Calor. Também mostrarei como criá-los usando o software R."
  },
  {
    "objectID": "artigos/mapas-calor/index.html#o-que-é-um-mapa-de-calor",
    "href": "artigos/mapas-calor/index.html#o-que-é-um-mapa-de-calor",
    "title": "Mapas de Calor",
    "section": "O que é um mapa de calor?",
    "text": "O que é um mapa de calor?\nOs Mapas de calor são representações gráficas que utilizam uma escala de cores para representar diferentes valores. Exemplificando, se tenho um conjuto de dados sobre a alimentação nos países europeus da década de 70 e quero construir um mapa de calor com ele, onde a escala de cores se intensifica conforme a média do consumo de determinado alimento aumenta, obtenho o seguinte mapa:\n\n\n\n\n\n\n\n\n\nMapas de calor podem sem também usados em um sentido mais literal do seu nome, onde queremos observar a incidência de algo, em uma determinada região (temperatura média de um estado brasileiro, por exemplo)."
  },
  {
    "objectID": "artigos/mapas-calor/index.html#criando-mapas-de-calor-com-auxilo-do-rstudio",
    "href": "artigos/mapas-calor/index.html#criando-mapas-de-calor-com-auxilo-do-rstudio",
    "title": "Mapas de Calor",
    "section": "Criando mapas de calor com auxilo do RStudio",
    "text": "Criando mapas de calor com auxilo do RStudio\nExistem várias maneiras de criar mapas de calor no R. Irei aprestentar duas delas: utilizando a função nativa heatmap() e, em seguida, a biblioteca ggplot2.\nTrabalharei com um conjunto de dados similar ao que usei para o mapa de calor do consumo de alimentos dos países europeus mostrado anteriormente, mas com dados simulados sobre a alimentação de 10 espécies diferentes de tubarões.\n\nUtilizando a função heatmap()\nO R nos fornece uma função nativa para criarmos mapas de calor. Uma coisa bem interessante é que, por default, a função realiza a clusterização do conjunto de dados tanto das variáveis presentes no eixo das abscissas (eixo x) quanto do eixo das ordenadas (eixo y), ou seja, ela agrupa os dados “mais parecidos” entre si.\nA seguir, apresentarei como criar o mapa de calor usando a função heatmap() e alterando essa clusterização que ela realiza.\nInicialmente, importarei o dataset criado com os dados fictícios.\n\ndados_tubaroes &lt;- read.csv(\"tubaroes_heatmap.csv\", header = TRUE)\ndados_tubaroes\n\n                especie salmao atum linguado bacalhau sardinha\n1        tubarao_branco    200  180      150      220      100\n2       tubarao_martelo    180  150      160      200      120\n3         tubarao_tigre    200  170      180      210      150\n4        tubarao_salmao    180  150      160      200      120\n5    tubarao_groelandia    200  180      160      220      150\n6  tubarao_galha_branca    180  150      160      200      120\n7  tubarao_branco_frade    190  160      170      210      130\n8          tubarao_azul    170  140      150      190      110\n9  tubarao_cabeca_chata    200  170      180      220      140\n10         tubarao_mako    220  190      200      240      160\n\n\nComo a função heatmap() pede que o objeto que contenha os dados seja uma matriz, farei a seguir uma pequena manipulação do conjunto de dados, onde colocarei os nomes das espécies de tubarões como nome das linhas e excluirei a coluna dados_tubaroes$especie, que não será mais necessária.\n\nrownames(dados_tubaroes) &lt;- dados_tubaroes$especie\ndados_tubaroes &lt;- dados_tubaroes[,-1]\ndados_tubaroes\n\n                     salmao atum linguado bacalhau sardinha\ntubarao_branco          200  180      150      220      100\ntubarao_martelo         180  150      160      200      120\ntubarao_tigre           200  170      180      210      150\ntubarao_salmao          180  150      160      200      120\ntubarao_groelandia      200  180      160      220      150\ntubarao_galha_branca    180  150      160      200      120\ntubarao_branco_frade    190  160      170      210      130\ntubarao_azul            170  140      150      190      110\ntubarao_cabeca_chata    200  170      180      220      140\ntubarao_mako            220  190      200      240      160\n\n\nAgora que os dados já estão do jeito que quero, posso finalmente usar a função heatmap(), lembrando que eles terão que estar em forma de matriz.\n\nheatmap(as.matrix(dados_tubaroes))\n\n\n\n\n\n\n\n\nComo tinha dito, a função fez a clusterização dos dados. Observe que ela também mostra o dendograma dessa clusterização. Para retirar a clusterização, usa-se o argumento Rowv = NA para a das linhas e Colv = NA para a das colunas, veja:\n\n#mapa de calor sem o dendograma das linhas\nheatmap(as.matrix(dados_tubaroes), Rowv = NA) \n\n\n\n\n\n\n\n#mapa de calor sem o dendograma das colunas\nheatmap(as.matrix(dados_tubaroes), Colv = NA) \n\n\n\n\n\n\n\n#mapa de calor sem os dendogramas\nheatmap(as.matrix(dados_tubaroes), Rowv = NA, Colv = NA) \n\n\n\n\n\n\n\n\n\n\nUtilizando a biblioteca ggplot2\nA biblioteca ggplot2 dá diversas funções interessantes para visualização de dados. Com ela é possivel criar os mais variados tipos de gráficos de maneira muito elegante (e eles ficam lindos!).\nAntes de qualquer coisa, irei fazer a importação da biblioteca.\n\nlibrary(ggplot2)\n\nPara criar o mapa de calor com ela, preferi fazer uma pequena alteração na estrutura do data frame, pois antes eles estavam dispostos de uma maneira que se assemelha a uma matriz e para conseguir usá-los com o ggplot preciso que eles estejam em um formato do tipo tabela. Apesar dessa alteração, o conteúdo dos dados permanece o mesmo. Veja como ficaram as primeiras 10 linhas:\n\ndados_tubaroes &lt;- read.csv(\"tubaroes_ggplot.csv\", header = TRUE)\ndados_tubaroes[1:10,]\n\n           especie peixe_consumido consumo_diario\n1   tubarao_branco          salmao            200\n2   tubarao_branco            atum            180\n3   tubarao_branco        linguado            150\n4   tubarao_branco        bacalhau            220\n5   tubarao_branco        sardinha            100\n6  tubarao_martelo          salmao            180\n7  tubarao_martelo            atum            150\n8  tubarao_martelo        linguado            160\n9  tubarao_martelo        bacalhau            200\n10 tubarao_martelo        sardinha            120\n\n\nAgora posso criar o gráfico. No eixo x quero que fiquem as espécies de que os tubarões consomem, e no eixo y o nome das espécies de tubarões que aparecem no conjunto de dados que estou trabalhando. O gráfico será preenchido com a quantidade de consumo de cada peixe por espécie. E utilizarei a geometria geom_tile para dar forma ao mapa de calor.\n\nggplot(data = dados_tubaroes, mapping = aes(x = peixe_consumido, \n                                            y = especie, \n                                            fill = consumo_diario))+\n  geom_tile()\n\n\n\n\n\n\n\n\nO ggplot me permite fazer diversas alterações no gráfico, irei fazer algumas a seguir para mostrar um pouquinho das diversas possibilidades que ele dá. Fique à vontade para explorar o pacote e descobrir outras alterações possiveis.\n\nggplot(data = dados_tubaroes, mapping = aes(x = peixe_consumido, \n                                            y = especie, \n                                            fill = consumo_diario))+\n  geom_tile()+\n  scale_fill_gradient(low = \"lightskyblue1\", high = \"dodgerblue4\")+\n  guides(fill = guide_colourbar(barwidth = 0.75, barheight = 10))+\n  labs(title = \"Alimentação diária de tubarões\",\n       subtitle = \"(em gramas)\",\n       x = \"Peixes\",\n       y = \"Espécie de tubarão\")+\n  theme_minimal()+\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nOlha como ficou bonitinho!\nEspero que tenha gostado dos mapas de calor e ficado interessado em criar alguns. Agora é sua vez de colocar os conhecimentos em prática."
  },
  {
    "objectID": "artigos/cv/index.html",
    "href": "artigos/cv/index.html",
    "title": "Uma medida regularmente esquecida",
    "section": "",
    "text": "Introdução\nVocê é técnico de um time de futebol e tem que escalar uma nova jogadora no seu time titular para o início da pré-temporada. Seu coordenador te dá duas opções de meio-campistas recém promovidas da base, Alicia e Alexia. Ele também te fornece um histórico dos últimos jogos informando a participações em gols:\nAlicia : 1,1,1,2,1,1,3  Alexia : 3,0,2,4,0,0\nE assim você entra no seguinte dilema: qual atleta escolher, dado que ambas são bem semelhantes no modo de jogo? Qual critério adotar?\n\n\nMedidas\nUm primeiro indicador que vem em mente seria a soma de participações em gols. Porém, como uma das atletas tem um jogo a mais, ela leva vantagem. Uma boa alternativa para isso seria aplicar a média.\nSendo assim, a média de participações em gols da Alicia e Alexia, são de 1.43 e 1.5 gols, respectivamente. Podemos perceber que a Alexia obteve um indicador maior, mas que difere pouco da Alicia. Inicialmente poderíamos optar por escalar a Alexia.\nNo entanto, quando olhamos a progressão de participação de gols, jogo a jogo, percebemos uma certa diferença na distribuição das duas, onde Alicia aparenta oscilar pouco ao decorrer das partidas, entregando no mínimo uma participação em gol por jogo. Já a Alexia se apresenta “8 ou 80” nessa amostra de jogos, atingindo uma partida com 4 contribuições diretas para gol. Por outro lado, em 3 partidas ela passou em branco.\n\n\nCálculo e Interpretação\nO cálculo do desvio padrão ajuda bastante para analisar a dispersão do desempenho de ambas. Porém, uma medida que nos fornece outra ótica sobre as participações em gols é o coeficiente de variação (CV), também conhecido como dispersão relativa. Essa métrica poderosa nada mais é que o desvio padrão amostral dividido pela média amostral, e quando desejamos ter uma ideia em percentual podemos multiplicar esse indicador por 100.\nO Coeficiente de Variação (CV) pode ser interpretado de diversas maneiras. Primeiramente, quanto menor o CV, mais representativa é a média para a amostra. Em contrapartida, o CV também pode ser visto como um indicador de regularidade: valores altos indicam maior irregularidade nos dados da amostra, enquanto valores baixos sugerem maior regularidade.\n\n\nBenefícios\nAlém de ser uma medida que possui alta interpretabilidade, há um benefício de flexibilidade, que pode ser visto em dois sentidos:\nComparando a rentabilidade nos últimos 5 meses das ações de dois investidores:\nPrimoRico : 7800,4500,15000,5500,9800  PrimoPobre : 100,70,450,190,230\nA pergunta que pode surgir é: qual a carteira possui mais regularidade ao longo dos meses observados? A propriedade que observamos é que podemos comparar duas amostras com grande diferença de magnitude, uma vez que um dos investidores pode fazer maiores aportes. Como as dispersões relativas do Primo Rico e do Primo Pobre são de aproximadamente 49% e 72%, respectivamente, podemos afirmar que a carteira de investimento do Primo Rico é mais “confiável” ao longo do período analisado.\nEntretanto, há um outro benefício muito legal do CV, a possibilidade de comparar maçã com banana. Imagine que um dos seus jogadores principais se lesiona no aquecimento antes de uma partida importante. Seu auxiliar técnico te informa duas possibilidades de formações alternativas, uma com um zagueiro a mais e outra com mais um atacante. Na prática ambas as possibilidades não mudam muito a mentalidade de como o time vai jogar. Porém, a regularidade dos atletas seria muito relevante para manter o equilíbrio coletivo e a confiança da comissão, de maneira que quem for escolhido vai desempenhar aquilo que tem feito em média nos últimos jogos que fez parte do time.\nDito isso, o seu analista fornece estatísticas dos últimos 4 jogos de três fundamentos principais das suas duas opções. Para o zagueiro, temos os desarmes (D), percentual de disputas de bola ganhas (PDG) e interceptações (I). Já para o atacante, temos participações em gols (PG), passes decisivos (PD) e taxa de dribles bem-sucedidos (TDS).\nD : 3,1,0,0  PDG : 85,95,76,66  I : 2,4,2,1  PG : 0,3,0,1  PD : 1,2,0,0  TDS : 33,50,100,22\nAntes de falarmos do resultado, enfatizamos que estamos comparando a regularidade de atletas de posições totalmente diferentes, evidenciando o poder do uso dessa métrica para esse tipo de interpretação. Uma ideia para comparar os dois jogadores será calcular o CV para cada uma das métricas, e por fim, realizar uma média do CV dos indicadores condizentes com cada um dos jogadores. Temos que a média do CV do zagueiro foi de 71%, e do atacante de 112%, logo o jogador mais regular dentre as métricas definidas foi o defensor, e assim o técnico pode tomar sua decisão.\n\n\nConclusão\nHá outros modos de explorar o Coeficiente de Variação, e isso vai de encontro com a criatividade do entusiasta dos dados; dito isso, que o CV não seja tão esquecido e que seja utilizado com mais regularidade!"
  },
  {
    "objectID": "dashboards.html",
    "href": "dashboards.html",
    "title": "Recent Earthquakes in Aotearoa New Zealand",
    "section": "",
    "text": "library(tidyverse)\nlibrary(httr2)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(gt)\n# Get data from GeoNet\n# MMI = 3, weak or above\nreq &lt;- request(\"https://api.geonet.org.nz/quake?MMI=3\") |&gt; \n  req_headers(\"Accept\"=\"application/vnd.geo+json\")\nresp &lt;- req_perform(req)\nrecent_quakes &lt;- resp |&gt; \n  resp_body_string() |&gt; \n  st_read(quiet = TRUE)\n# Prettier times and dates\nrecent_quakes &lt;- recent_quakes |&gt; \n  arrange(desc(time)) |&gt; \n  mutate(\n    time = force_tz(time, \"Pacific/Auckland\"),\n    pretty_time = format(time, \"%I:%M %p\"),\n    days_ago = today(tzone = \"Pacific/Auckland\") - date(time),\n    days_ago = case_when(\n      days_ago == 0 ~ \"Today\",\n      days_ago == 1 ~ \"Yesterday\",\n      TRUE ~ paste0(days_ago, \" days ago\")\n    )\n  )\nnow_nz &lt;- now(tzone = \"Pacific/Auckland\")\nlast_24 &lt;- recent_quakes |&gt; filter(time &gt; (now_nz - hours(24)))\nn_24 &lt;- nrow(last_24)\nhours_last &lt;- round(difftime(now_nz, recent_quakes$time[1], units = \"hours\"))\nmag_pal &lt;- colorBin(\"inferno\", domain = 1:8, bins = c(0:5, 8))\n\nquake_map &lt;- recent_quakes |&gt; \n  leaflet() |&gt; \n  addCircleMarkers(\n    color = ~ mag_pal(magnitude),\n    stroke = FALSE,\n    fillOpacity = 0.5,\n    radius = ~ scales::rescale(sqrt(magnitude), c(1, 10)),\n    label = ~ paste(\n      date(time), pretty_time, \"&lt;br/&gt;\",\n      \"Magnitude:\", round(magnitude, 1), \"&lt;br/&gt;\", \n      \"Depth:\",  round(depth), \" km\"\n      ) |&gt; map(html),\n    labelOptions = c(textsize = \"15px\")) |&gt; \n  addLegend(title = \"Magnitude\", colors = mag_pal(0:5), labels = c(\"&lt;1\", 1:4,\"&gt;5\")) |&gt; \n  addTiles(\"http://services.arcgisonline.com/arcgis/rest/services/Canvas/World_Light_Gray_Base/MapServer/tile/{z}/{y}/{x}\", options = tileOptions(minZoom = 5, maxZoom = 10))\nmag_hist &lt;- recent_quakes |&gt; \n  ggplot(aes(x = magnitude)) +\n  geom_histogram()\ntimeline &lt;- recent_quakes |&gt; \n  ggplot(aes(x = time, y = 0)) +\n  geom_point()\n# Create n most recent table\nn &lt;- 10\ntop_n &lt;- recent_quakes |&gt; \n  slice(1:n) |&gt; \n  as.data.frame() |&gt; \n  select(magnitude, days_ago, pretty_time, locality, depth) \n\ntop_n_table &lt;- top_n |&gt; \n  gt() |&gt; \n  cols_label(\n    days_ago = \"\",\n    locality = \"Location\",\n    magnitude = \"Magnitude\",\n    depth = \"Depth\",\n    pretty_time = \"\"\n  ) |&gt; \n  fmt_integer(\n    columns = depth, \n    pattern = \"{x} km\"\n  ) |&gt; \n  fmt_number(\n    columns = magnitude,\n    decimals = 1\n  ) |&gt; \n  data_color(\n    columns = \"magnitude\",\n    fn = mag_pal\n  ) |&gt;\n  tab_header(\n    title = md(\"**Last 10 Earthquakes**\")\n  ) |&gt; \n  tab_source_note(\n    source_note = md(paste(\"Retrieved from the [GeoNet API](https://api.geonet.org.nz/) at\", format(now_nz, \"%Y/%m/%d %H:%M %Z\")))\n  )"
  },
  {
    "objectID": "dashboards.html#column",
    "href": "dashboards.html#column",
    "title": "Recent Earthquakes in Aotearoa New Zealand",
    "section": "Column",
    "text": "Column\n\nRow\n\nlist(\n  icon = \"stopwatch\",\n  color = \"primary\",\n  value = as.numeric(hours_last)\n)\n\n$icon\n[1] \"stopwatch\"\n\n$color\n[1] \"primary\"\n\n$value\n[1] 18\n\n\n\nlist(\n  icon = \"activity\",\n  color = \"secondary\",\n  value = n_24\n)\n\n$icon\n[1] \"activity\"\n\n$color\n[1] \"secondary\"\n\n$value\n[1] 1\n\n\n\n\nRow\n\ntop_n_table\n\n\n\n\n\n\n\nLast 10 Earthquakes\n\n\nMagnitude\n\n\nLocation\nDepth\n\n\n\n\n2.1\nYesterday\n02:16 PM\n5 km south-west of Oamaru\n5 km\n\n\n3.8\n2 days ago\n03:54 PM\n20 km east of Paraparaumu\n51 km\n\n\n3.3\n2 days ago\n02:19 AM\n30 km east of Matawai\n20 km\n\n\n2.4\n2 days ago\n01:21 AM\n5 km south of Dannevirke\n6 km\n\n\n3.0\n2 days ago\n12:20 AM\n20 km south-east of Hastings\n21 km\n\n\n2.3\n3 days ago\n10:58 PM\n5 km north-east of Ruatoria\n16 km\n\n\n2.4\n4 days ago\n06:01 AM\n10 km east of Ruatoria\n16 km\n\n\n3.3\n5 days ago\n11:40 AM\n40 km west of Wairoa\n29 km\n\n\n3.1\n5 days ago\n11:32 AM\n15 km north-east of Waipukurau\n10 km\n\n\n3.4\n5 days ago\n05:55 AM\n15 km east of Seddon\n11 km\n\n\n\nRetrieved from the GeoNet API at 2024/06/12 08:08 NZST"
  },
  {
    "objectID": "dashboards.html#column-1",
    "href": "dashboards.html#column-1",
    "title": "Recent Earthquakes in Aotearoa New Zealand",
    "section": "Column",
    "text": "Column\n\nquake_map\n\nInput to asJSON(keep_vec_names=TRUE) is a named vector. In a future version of jsonlite, this option will not be supported, and named vectors will be translated into arrays instead of objects. If you want JSON object output, please use a named list instead. See ?toJSON."
  },
  {
    "objectID": "artigos_fila/estatistica-basquete/index.html",
    "href": "artigos_fila/estatistica-basquete/index.html",
    "title": "Estatística no Basquete: Transformando a Escolha do MVP",
    "section": "",
    "text": "Atualmente, é perceptível cada vez mais que o uso da estatística no esporte resulta em melhorias tanto nas performances individuais quanto na coletiva. Um exemplo disso é o aumento da procura dos times e atletas por analistas pois por meio da análise de dados, eles podem identificar padrões de desempenho de suas equipes e dos adversários, ajustar suas táticas de acordo com as tendências observadas e maximizar suas chances de vitória. Agora destacarei a importância deste uso, o seu impacto e algo que nem sempre é percebido: a sua influência nas premiações individuais.\nMeu interesse por esse tema foi despertado recentemente ao ver um comentário de Shaquille O’Neal, um jogador aposentado e atualmente comentarista, criticando a escolha de Nikola Jokic, jogador em análise hoje, como ganhador do prêmio de MVP. Esse post me fez refletir sobre como o prêmio de MVP se transformou e como tem se tornado cada vez mais dependente de estatísticas abrangentes na escolha do vencedor."
  },
  {
    "objectID": "artigos_fila/estatistica-basquete/index.html#introdução",
    "href": "artigos_fila/estatistica-basquete/index.html#introdução",
    "title": "Estatística no Basquete: Transformando a Escolha do MVP",
    "section": "",
    "text": "Atualmente, é perceptível cada vez mais que o uso da estatística no esporte resulta em melhorias tanto nas performances individuais quanto na coletiva. Um exemplo disso é o aumento da procura dos times e atletas por analistas pois por meio da análise de dados, eles podem identificar padrões de desempenho de suas equipes e dos adversários, ajustar suas táticas de acordo com as tendências observadas e maximizar suas chances de vitória. Agora destacarei a importância deste uso, o seu impacto e algo que nem sempre é percebido: a sua influência nas premiações individuais.\nMeu interesse por esse tema foi despertado recentemente ao ver um comentário de Shaquille O’Neal, um jogador aposentado e atualmente comentarista, criticando a escolha de Nikola Jokic, jogador em análise hoje, como ganhador do prêmio de MVP. Esse post me fez refletir sobre como o prêmio de MVP se transformou e como tem se tornado cada vez mais dependente de estatísticas abrangentes na escolha do vencedor."
  },
  {
    "objectID": "artigos_fila/estatistica-basquete/index.html#mvp-o-que-é-e-como-funciona",
    "href": "artigos_fila/estatistica-basquete/index.html#mvp-o-que-é-e-como-funciona",
    "title": "Estatística no Basquete: Transformando a Escolha do MVP",
    "section": "MVP: O que é e Como Funciona",
    "text": "MVP: O que é e Como Funciona\nO MVP, ou Most Valuable Player, é concedido anualmente ao jogador considerado o mais impactante para sua equipe durante a temporada regular da NBA. A escolha do MVP é feita por um painel de jornalistas e especialistas em basquete, que votam no jogador que consideram mais qualificado para o prêmio com base em uma série de critérios, incluindo estatísticas individuais, contribuições para o sucesso da equipe, liderança e impacto geral no jogo. Anteriormente, a escolha do MVP era baseada principalmente em estatísticas básicas, mas agora inclui diversas métricas avançadas."
  },
  {
    "objectID": "artigos_fila/estatistica-basquete/index.html#exemplo-de-algumas-dessas-métricas",
    "href": "artigos_fila/estatistica-basquete/index.html#exemplo-de-algumas-dessas-métricas",
    "title": "Estatística no Basquete: Transformando a Escolha do MVP",
    "section": "Exemplo de Algumas Dessas Métricas:",
    "text": "Exemplo de Algumas Dessas Métricas:\nPlayer Efficiency Rating (PER): Busca quantificar o impacto geral de um jogador em todas as áreas do jogo, considerando uma variedade de estatísticas individuais e ponderando-as de acordo com sua importância relativa.\nWin Shares (WS): Calcula a contribuição de um jogador para as vitórias de sua equipe ao longo da temporada, considerando sua contribuição ofensiva, defensiva e simplesmente por estar em quadra."
  },
  {
    "objectID": "artigos_fila/estatistica-basquete/index.html#análise-gráfica-dos-finalistas-do-prêmio-mvp",
    "href": "artigos_fila/estatistica-basquete/index.html#análise-gráfica-dos-finalistas-do-prêmio-mvp",
    "title": "Estatística no Basquete: Transformando a Escolha do MVP",
    "section": "Análise Gráfica dos Finalistas do Prêmio MVP",
    "text": "Análise Gráfica dos Finalistas do Prêmio MVP\nPara ilustrar a mudança no prêmio MVP devido ao aumento do peso das estatísticas, apresento uma análise gráfica com as principais estatísticas dos finalistas deste ano: Nikola Jokic, Shai Gilgeous-Alexander e Luka Doncic. Os dados que utilizei para construir e analisar esse gráfico foram obtidos do site basketball-reference e correspondem à temporada regular 2023-24 dos três jogadores. A análise visa contrapor a declaração de Shaq :“Quero parabenizá-lo, mas quero que você ouça isso de mim primeiro. Achei que o SGA deveria ter sido o MVP, isso não é desrespeito a você, mas parabéns.”, parece ser baseada em uma visão tradicional da premiação, que favorece os jogadores mais dominantes.\n\n\n\n\n\n\n\n\n\nÉ possível observando o gráfico ver que, na parte estatística, a escolha de Jokic como MVP não é um erro, pois podemos notar que, em todas as métricas, a diferença entre ele e os outros concorrentes não é significativa, liderando inclusive o PER e o WS, métricas que melhor refletem o impacto de um jogador em sua equipe."
  },
  {
    "objectID": "artigos_fila/estatistica-basquete/index.html#conclusão",
    "href": "artigos_fila/estatistica-basquete/index.html#conclusão",
    "title": "Estatística no Basquete: Transformando a Escolha do MVP",
    "section": "Conclusão",
    "text": "Conclusão\nQuis destacar como a estatística tem influenciado positivamente o basquete, não apenas melhorando o desempenho individual e coletivo, mas também reconhecendo jogadores com habilidades anteriormente subestimadas, o que contribui para o desempenho da equipe. Além disso, em minha opinião, o uso da estatística aumenta a transparência e objetividade na seleção do MVP. Ao considerar uma gama mais ampla de métricas, os eleitores do MVP podem tomar decisões mais informadas e justas, reconhecendo verdadeiramente o jogador mais valioso para sua equipe e para a liga como um todo."
  },
  {
    "objectID": "teste_dashboard.html",
    "href": "teste_dashboard.html",
    "title": "Untitled",
    "section": "",
    "text": "plot(iris)"
  },
  {
    "objectID": "artigos_fila/clusterizacao/index.html",
    "href": "artigos_fila/clusterizacao/index.html",
    "title": "Clusterização K-means e uma aplicação com mapas",
    "section": "",
    "text": "Uma breve introdução\nDentro das muitas definições sobre o que é a Estatística, uma abordagem é tentar entendê-la como uma ciência capaz de utilizar métodos científicos para estudar e tirar conclusões sobre uma população a partir de suas informações.\nAo estudar uma população ou um conjunto de dados, uma das primeiras perguntas que podemos fazer para nos ajudar a conhecer nossos dados é: será que os elementos desse conjunto possuem semelhanças entre si? Se sim, será que essas semelhanças são suficientes para dividi-los em grupos?\nDentro do aprendizado de máquina (ou machine learning), uma ferramenta poderosa para nos auxiliar com essas respostas é a técnica de clusterização, do inglês clustering, que consiste em um modelo de aprendizado não supervisionado que utiliza técnicas computacionais para separar um conjunto de dados em grupos, baseando-se em suas características similares.\n\n\nMas afinal, o que é aprendizado de máquina?\nPodemos definir aprendizado de máquina como um campo de estudo dentro da inteligência artificial que desenvolve algoritmos, com o objetivo de capacitar a máquina a aprender com os dados e identificar padrões. De certa forma, é como se estivéssemos ensinando nosso computador a replicar o pensamento analítico humano.\nPodemos dividir o aprendizado de máquina em quatro subcategorias: aprendizagem supervisionada, semi-supervisionada, não supervisionada e por reforço. No caso da clusterização, a técnica se encaixa no modelo de aprendizagem não supervisionada. Esse modelo se caracteriza por utilizar conjuntos de dados não rotulados, ou seja, o modelo deve encontrar sozinho os padrões contidos nos dados, já que não há uma variável resposta.\nExistem diversos algoritmos de clusterização, uma vez que cada problema pode exigir uma abordagem diferente baseado em suas características. No nosso caso, utilizaremos o algoritmo chamado K-means. Esse modelo consiste no particionamento dos elementos em k grupos (clusters), em que cada observação pertence ao grupo com a média mais próxima.\n\n\n\n\n\n\n\nAplicação utilizando o conjunto USArrests\nAgora que já entendemos o que é aprendizado de máquina e o que é clusterização, aplicaremos esse conhecimento utilizando o conjunto de dados USArrests. Este conjunto de dados está disponível na biblioteca nativa do R e tem informações sobre as taxas de prisões por crimes violentos em cada estado dos Estados Unidos em 1973.\nPrimeiro, importamos o conjunto de dados e o salvamos em um dataframe que chamaremos de dados.  Depois de importar, precisamos fazer a padronização dos dados, ou seja, colocar todas as variáveis na mesma ordem de grandeza. Para isso, usamos a função scale.\n\ndata(\"USArrests\")\ndados &lt;- USArrests\ndados_padronizados &lt;- scale(dados)\n\nCom os dados devidamente padronizados, agora podemos aplicar o algoritmo K-means e dividir nosso conjunto em clusters (grupos). A função que usaremos para isso se chama kmeans() e possui os seguintes argumentos:\n\n‘x’: conjunto de dados a ser agrupado (dataframe ou matriz);\n‘centers’: número de clusters desejado (‘k’);\n‘iter.max’: número máximo de iterações ;\n‘nstart’: número de vezes que o algoritmo será executado com diferentes pontos de partida (aleatórios).\n\nPara esse conjunto de dados, tomaremos nosso ’k’ como 5, o número de iterações como 20 e ’nstart’ como 50.\n\nclusterizacao &lt;- kmeans(x = dados_padronizados, centers = 5, iter.max = 20, nstart = 50)\nclusterizacao\n\nEsse código nos retorna as principais informações sobre o nosso agrupamento, sendo elas: a quantidade de clusters e seus respectivos tamanhos, as médias das variáveis para cada cluster, o vetor indicador da classificação de cada observação, a soma de quadrados dos clusters e uma lista de todos os componentes disponíveis.\nO componente que mais nos interessa é o vetor indicando a qual cluster (de 1 a 5) cada observação do conjunto de dados pertence. Para isso, usamos o código a seguir:\n\nclusterizacao$cluster\n\n       Alabama         Alaska        Arizona       Arkansas     California \n             4              1              1              3              1 \n      Colorado    Connecticut       Delaware        Florida        Georgia \n             1              2              2              1              4 \n        Hawaii          Idaho       Illinois        Indiana           Iowa \n             2              5              1              3              5 \n        Kansas       Kentucky      Louisiana          Maine       Maryland \n             3              3              4              5              1 \n Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n             2              1              5              4              3 \n       Montana       Nebraska         Nevada  New Hampshire     New Jersey \n             3              3              1              5              2 \n    New Mexico       New York North Carolina   North Dakota           Ohio \n             1              1              4              5              2 \n      Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n             3              3              2              2              4 \n  South Dakota      Tennessee          Texas           Utah        Vermont \n             5              4              1              2              5 \n      Virginia     Washington  West Virginia      Wisconsin        Wyoming \n             3              2              5              5              3 \n\n\nAgora que conseguimos agrupar os estados, criaremos uma coluna em nosso dataframe chamada cluster contendo a classificação de cada observação. Também usaremos a função as.factor() para transformar o vetor em um fator, ou seja, em um dado categórico.\n\ndados$cluster &lt;- as.factor(clusterizacao$cluster)\n\nUma vez que o nosso dataframe possui a coluna indicando os clusters, podemos separar o nosso conjunto de dados em subgrupos, para que possamos ver de forma mais eficiente quais são os estados pertencentes a cada grupo.\n\ncluster1 &lt;- dados[dados$cluster == 1, ]\ncluster2 &lt;- dados[dados$cluster == 2, ]\ncluster3 &lt;- dados[dados$cluster == 3, ]\ncluster4 &lt;- dados[dados$cluster == 4, ]\ncluster5 &lt;- dados[dados$cluster == 5, ]\n\nrownames(cluster1)\n\n [1] \"Alaska\"     \"Arizona\"    \"California\" \"Colorado\"   \"Florida\"   \n [6] \"Illinois\"   \"Maryland\"   \"Michigan\"   \"Nevada\"     \"New Mexico\"\n[11] \"New York\"   \"Texas\"     \n\nrownames(cluster2)\n\n [1] \"Connecticut\"   \"Delaware\"      \"Hawaii\"        \"Massachusetts\"\n [5] \"New Jersey\"    \"Ohio\"          \"Pennsylvania\"  \"Rhode Island\" \n [9] \"Utah\"          \"Washington\"   \n\nrownames(cluster3)\n\n [1] \"Arkansas\" \"Indiana\"  \"Kansas\"   \"Kentucky\" \"Missouri\" \"Montana\" \n [7] \"Nebraska\" \"Oklahoma\" \"Oregon\"   \"Virginia\" \"Wyoming\" \n\nrownames(cluster4)\n\n[1] \"Alabama\"        \"Georgia\"        \"Louisiana\"      \"Mississippi\"   \n[5] \"North Carolina\" \"South Carolina\" \"Tennessee\"     \n\nrownames(cluster5)\n\n [1] \"Idaho\"         \"Iowa\"          \"Maine\"         \"Minnesota\"    \n [5] \"New Hampshire\" \"North Dakota\"  \"South Dakota\"  \"Vermont\"      \n [9] \"West Virginia\" \"Wisconsin\"    \n\n\nAgora que sabemos a qual grupo pertence cada estado, podemos começar a pensar em alguns questionamentos interessantes. Além das taxas de prisões, o que será que esses estados agrupados no mesmo cluster possuem em comum? Será que são estados geograficamente próximos uns dos outros? Será que dividem semelhanças em aspectos socioeconômicos ou no contexto histórico?\nCom essas perguntas em mente, um recurso gráfico muito útil para nos ajudar a visualizar esse agrupamento e aprofundar nossa análise é o uso de mapas. Nesse caso, podemos utilizar um mapa dos Estados Unidos para colorir os estados de cada cluster encontrado usando cores diferentes.\n\n\nAplicação utilizando mapas\nPara montar nosso mapa, precisamos de algumas ferramentas e de informações sobre as coordenadas geográficas de cada estado. Então, utilizaremos as seguintes bibliotecas: \n\n‘ggplot2’: própria para a visualização de dados e construção de gráficos;\n‘dplyr’: utilizada para auxiliar na manipulação de dados e dataframes;\n‘maps’: contém dados sobre a latitude e longitude de contornos de continentes, países, estados e condados do mundo.\n\n\nlibrary(maps)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nPrimeiro, criaremos a variável dados_geograficos contendo as coordenadas dos estados e adicionaremos uma nova coluna ao dataframe chamada state, contendo os nomes dos estados em letra minúscula. Então, utilizaremos a função left_join() do pacote dplyr para que possamos juntar as informações geográficas ao nosso conjunto.\n\ndados_geograficos &lt;- map_data(\"state\")\ndados$state &lt;- tolower(rownames(dados))\ndados &lt;- left_join(dados, dados_geograficos, by = c(\"state\" = \"region\"))\n\nAgora, utilizando a função geom_polygon() no ggplot, conseguimos desenhar um polígono que dará forma ao nosso mapa. Para isso, usamos as coordenadas sobre latitude e longitude dos contornos dos estados obtidas anteriormente pela função map_data().\n\nggplot(data = dados, mapping = aes(x = long, y = lat, group = group, fill = cluster)) +\n  geom_polygon() +\n  labs(title = \"Clusterização dos estados norte-americanos\")\n\n\n\n\n\n\n\n\nConhecendo um pouco mais sobre o ggplot e explorando suas diversas funções, podemos modificar esse gráfico de acordo com nossas preferencias, mudando as cores ou até mesmo o tipo de projeção utilizada no mapa, como no exemplo a seguir.\n\nggplot(data = dados, mapping = aes(x = long, y = lat, group = group, fill = cluster)) +\n  geom_polygon(color = \"gray90\", linewidth = 0.1) +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  scale_fill_brewer(palette = \"Set2\")+\n  theme_minimal()+\n  theme(legend.position = \"bottom\")+\n  labs(x= NULL, y= NULL, title = \"Clusterização dos estados norte-americanos\")\n\n\n\n\n\n\n\n\nFinalmente, com o auxílio do mapa, podemos começar a pensar em respostas para aquelas perguntas que fizemos sobre o conjunto de dados. Em uma análise mais superficial, vemos que a posição geográfica parece de fato ser um fator muito importante a ser considerado quando tentamos entender os motivos por trás dos clusters obtidos. Com essa nova suposição sobre nossos dados, passamos a enxergar nosso problema por um novo ângulo, o que nos permite aprofundar a compreensão dos dados e formular novas hipóteses.\nConsiderar apenas o mapa pode não nos trazer conclusões concretas sobre o agrupamento ou sobre o conjunto de dados, mas nos dá uma importante ferramenta que auxilia no entendimento dos dados e enriquece muito a nossa análise."
  },
  {
    "objectID": "artigos_fila/guia-teste-hipoteses/index.html",
    "href": "artigos_fila/guia-teste-hipoteses/index.html",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "",
    "text": "“Uma moeda foi lançada 200 vezes e foram obtidas 127 coroas. Suspeita-se que a moeda seja desonesta para coroa, ou seja, o resultado coroa tem maior probabilidade de ocorrer. Verifique a desonestidade da moeda com no máximo 5% de chance de a conclusão ser errada.”\nExercícios como este são comuns quando queremos testar uma hipótese. Mesmo para aqueles que já têm familiaridade com o assunto surgem várias perguntas ao olhar para o exemplo: “Por onde eu começo?” “Qual é a minha hipótese inicial?” “O que ele quer saber?” “Qual teste devo usar?” entre outras. Esta pesquisa tem como objetivo servir de guia para simplificar a identificação e interpretação dessas questões ajudando a saber exatamente o que fazer em cada situação."
  },
  {
    "objectID": "artigos_fila/guia-teste-hipoteses/index.html#definição-de-hipótese-nula-e-alternativa",
    "href": "artigos_fila/guia-teste-hipoteses/index.html#definição-de-hipótese-nula-e-alternativa",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "Definição de hipótese nula e alternativa",
    "text": "Definição de hipótese nula e alternativa\nO primeiro passo em um teste de hipótese é definir as hipóteses nula e alternativa. Uma hipótese nula representada por H₀ é a hipótese inicial ou a suposição padrão sobre uma situação. No caso da moeda mencionada anteriormente a primeira ideia ao estar em contato com uma moeda é esperar que ela seja honesta. Portanto a H₀ : P(c) = 0.5 onde P(c) representa a probabilidade de obter coroa.\nA hipótese alternativa representada por H₁ é aquilo que se suspeita ou deseja testar. No exemplo da moeda suspeitamos que a moeda seja desonesta favorecendo o resultado coroa. Assim a hipótese alternativa é formulada como H₁ : P(c) &gt; 0.5."
  },
  {
    "objectID": "artigos_fila/guia-teste-hipoteses/index.html#escolha-do-teste-estatístico",
    "href": "artigos_fila/guia-teste-hipoteses/index.html#escolha-do-teste-estatístico",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "Escolha do teste estatístico",
    "text": "Escolha do teste estatístico\nO segundo passo é escolher o teste estatístico adequado. Triola no livro “Introdução a estatística” disponibiliza uma tabela mostrando qual tipo de teste usar em cada caso:\n\n\n\n\n\n\n\n\n\nParâmetro\nDistribuição Amostral\nRequisitos\nEstatística de Teste\n\n\n\n\nProporção\nNormal (Z)\nnp ≥ 5 e nq ≥ 5\n\\[\\begin{equation}z = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} \\end{equation}\\]\n\n\nMédia (σ desconhecido)\nt-student\nPopulação normalmente distribuída ou n &gt; 30\n\\[\\begin{equation} t = \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\end{equation}\\]\n\n\nMédia (σ conhecido)\nNormal (Z)\nPopulação normalmente distribuída ou n &gt; 30\n\\[\\begin{equation} z = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\end{equation}\\]\n\n\nDesvio-padrão σ ou variância σ²\nχ²\nPopulação normalmente distribuída\n\\[\\begin{equation} \\chi^2 = \\frac{(n - 1)s^2}{\\sigma^2} \\end{equation}\\]\n\n\n\nNo exemplo da moeda queremos estudar a proporção de coroas obtidas nos lançamentos. Portanto o teste apropriado a ser utilizado é o teste z para proporções. Para garantir que este teste seja válido é essencial verificar se os requisitos do teste são atendidos.\nPara um teste z de proporções precisamos assegurar que tanto np quanto nq sejam maiores ou iguais a 5.\n\nnp representa o número esperado de sucessos onde n é o tamanho da amostra e p é a proporção esperada de sucessos.\nnq representa o número esperado de fracassos onde q = 1 − p.\n\nNo caso da moeda assumindo que ela é honesta a probabilidade de obter coroa é 0.5 e a probabilidade de obter cara é também 0.5.\nDado que a moeda foi lançada 200 vezes (n=200):\n\\[\\begin{equation}\n200 \\times 0.5 = 100\n\\end{equation}\\] Tanto np quanto nq são iguais a 100 que é muito maior que 5. Portanto as condições np ≥ 5 e nq ≥ 5 são cumpridas. Isso confirma que podemos usar o teste z para proporções de forma válida neste caso.\n\\[\\begin{equation}\nz = \\frac{127/200 - 0.5}{\\sqrt{0.5 \\times (1-0.5) / 200}} \\approx 3.81\n\\end{equation}\\]"
  },
  {
    "objectID": "artigos_fila/guia-teste-hipoteses/index.html#determinação-do-valor-crítico",
    "href": "artigos_fila/guia-teste-hipoteses/index.html#determinação-do-valor-crítico",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "Determinação do valor crítico",
    "text": "Determinação do valor crítico\nO valor crítico é um ponto de corte que nos ajuda a decidir se devemos rejeitar a hipótese nula. Ele é determinado pelo nível de significância (α) do teste, que é a probabilidade máxima de cometer um erro do tipo I (rejeitar uma hipótese nula verdadeira).\nPara um nível de significância de 5% (α = 0.05), usamos tabelas de distribuição para encontrar o valor crítico correspondente. Existem dois tipos principais de testes estatísticos:\n\nTestes Unilaterais\nUm teste unilateral é usado quando a hipótese alternativa é direcionada, ou seja, espera-se que a estatística de teste seja significativamente maior ou menor que o valor da hipótese nula.\n\nTeste Unilateral à Direita: Usado quando a hipótese alternativa sugere que o parâmetro é maior que o valor da hipótese nula \\[\\begin{equation}H_1: \\mu &gt; \\mu_0\\end{equation}\\]\nTeste Unilateral à Esquerda: Usado quando a hipótese alternativa sugere que o parâmetro é menor que o valor da hipótese nula \\[\\begin{equation}H_1: \\mu &lt; \\mu_0\\end{equation}\\]\n\n\n\nTestes Bilaterais\nUm teste bilateral é usado quando a hipótese alternativa não é direcionada, ou seja, espera-se que a estatística de teste seja significativamente diferente (maior ou menor) do valor da hipótese nula.\n\nTeste Bilateral: Usado quando a hipótese alternativa sugere que o parâmetro é diferente do valor da hipótese nula \\[\\begin{equation}H_1: \\mu \\neq \\mu_0\\end{equation}\\]\n\nNo exemplo da moeda usaremos um teste unilateral à direita, já que a hipótese alternativa é P(c) &gt; 0.5.\n\nO valor crítico da distribuição normal padrão é aproximadamente 1.645."
  },
  {
    "objectID": "artigos_fila/guia-teste-hipoteses/index.html#tomada-de-decisão",
    "href": "artigos_fila/guia-teste-hipoteses/index.html#tomada-de-decisão",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "Tomada de decisão",
    "text": "Tomada de decisão\nSe o valor resultante do teste for maior que o valor crítico rejeitamos a hipótese nula. No nosso caso como 3.81 &gt; 1.645 rejeitamos a hipótese de que a moeda é justa. Portanto concluímos com um nível de significância de 5% que a moeda não é honesta."
  },
  {
    "objectID": "artigos/modelos-recomendacao/index.html",
    "href": "artigos/modelos-recomendacao/index.html",
    "title": "As melhores recomendações",
    "section": "",
    "text": "No mundo atual, nossa vida esta cada vez mais conectada com aplicativos e rede sociais, seja para otimizar nossa produtividade ou atender nossas necessidades sociais. Com isso, surge o aprimoramento do Mecanismos de Recomendação em que aumenta potencialmente a chance de a gente gostar de um conteúdo aleatório, com base nos conteúdos ou pessoas que vemos/seguimos, e assim, ficarmos mais tempo ou preferir uma plataforma específica.\nEu, particularmente, prefiro o YouTube Music quando comparado ao Spotify justamente pelas recomendações de um atender melhor meus desejos em detrimento do outro. Fato a se levar em conta é que utilizo o YouTube a um tempo consideravelmente maior o que pode influenciar em muito as recomendações, já que sua “base de dados sobre mim” é consideravelmente maior.\n\n\n\nExistem 3 tipos de Mecanismo de Recomendação: Filtragem Colaborativa, Filtragem baseada em conteúdo, e Sistema Hibrido. A Filtragem colaborativa, de uma maneira simples, é relacionada a outros usuários, os quais possuem gostos semelhantes ao meu. Por exemplo se eu gosto de jogos FPS como CSGO, Valorant e Battlefield um outro usuário joga somente Valorant e Battlefield, CSGO, então, seria uma excelente recomendação a este usuário já que os gostos são parecidos.\n\n\n\nJá a Filtragem baseada em conteúdo, se baseia no conteúdo (literalmente), utilizando o mesmo exemplo anterior, Valorant seria uma boa recomendação para quem gosta de CSGO, já que, ambos são jogos de FPS e com dinâmicas parecidas. O Sistema Híbrido seria uma mistura dos dois, gerando assim uma diversidade de produtos e um sistema mais robusto e com melhor precisão.\n\n\n\nUm caso que merece bastante destaque é o TikTok, já que seus infinitos vídeos curtos nos prendem em sua plataforma, e com a recomendação correta faz a gente perder mais tempo do que gostaria. Ele utiliza interações, hashtags, localização entre outros para recomendar vídeos de uma forma muito rápida. Deu tão certo que hoje esse Algoritmo de Recomendação é vendido como um produto para outras empresas.\nSendo assim, pode-se perceber que hoje em todo momento de conexão com a internet temos um algoritmo nos observando, sendo necessário tanto para melhorar nossa produtividade, quanto para ajudar a encontrar a música que a gente tanto gosta de uma maneira mais fácil. Para as empresas, cria-se a necessidade de desenvolvimento nesse setor, já que o investimento retorna como lucro nas vendas."
  },
  {
    "objectID": "artigos/modelos-recomendacao/index.html#introdução",
    "href": "artigos/modelos-recomendacao/index.html#introdução",
    "title": "As melhores recomendações",
    "section": "",
    "text": "No mundo atual, nossa vida esta cada vez mais conectada com aplicativos e rede sociais, seja para otimizar nossa produtividade ou atender nossas necessidades sociais. Com isso, surge o aprimoramento do Mecanismos de Recomendação em que aumenta potencialmente a chance de a gente gostar de um conteúdo aleatório, com base nos conteúdos ou pessoas que vemos/seguimos, e assim, ficarmos mais tempo ou preferir uma plataforma específica.\nEu, particularmente, prefiro o YouTube Music quando comparado ao Spotify justamente pelas recomendações de um atender melhor meus desejos em detrimento do outro. Fato a se levar em conta é que utilizo o YouTube a um tempo consideravelmente maior o que pode influenciar em muito as recomendações, já que sua “base de dados sobre mim” é consideravelmente maior."
  },
  {
    "objectID": "artigos/modelos-recomendacao/index.html#filtragem-por-usuário-ou-colaborativa",
    "href": "artigos/modelos-recomendacao/index.html#filtragem-por-usuário-ou-colaborativa",
    "title": "As melhores recomendações",
    "section": "",
    "text": "Existem 3 tipos de Mecanismo de Recomendação: Filtragem Colaborativa, Filtragem baseada em conteúdo, e Sistema Hibrido. A Filtragem colaborativa, de uma maneira simples, é relacionada a outros usuários, os quais possuem gostos semelhantes ao meu. Por exemplo se eu gosto de jogos FPS como CSGO, Valorant e Battlefield um outro usuário joga somente Valorant e Battlefield, CSGO, então, seria uma excelente recomendação a este usuário já que os gostos são parecidos."
  },
  {
    "objectID": "artigos/modelos-recomendacao/index.html#filtragem-de-conteúdo-e-híbrida",
    "href": "artigos/modelos-recomendacao/index.html#filtragem-de-conteúdo-e-híbrida",
    "title": "As melhores recomendações",
    "section": "",
    "text": "Já a Filtragem baseada em conteúdo, se baseia no conteúdo (literalmente), utilizando o mesmo exemplo anterior, Valorant seria uma boa recomendação para quem gosta de CSGO, já que, ambos são jogos de FPS e com dinâmicas parecidas. O Sistema Híbrido seria uma mistura dos dois, gerando assim uma diversidade de produtos e um sistema mais robusto e com melhor precisão."
  },
  {
    "objectID": "artigos/modelos-recomendacao/index.html#de-algoritmo-a-produto",
    "href": "artigos/modelos-recomendacao/index.html#de-algoritmo-a-produto",
    "title": "As melhores recomendações",
    "section": "",
    "text": "Um caso que merece bastante destaque é o TikTok, já que seus infinitos vídeos curtos nos prendem em sua plataforma, e com a recomendação correta faz a gente perder mais tempo do que gostaria. Ele utiliza interações, hashtags, localização entre outros para recomendar vídeos de uma forma muito rápida. Deu tão certo que hoje esse Algoritmo de Recomendação é vendido como um produto para outras empresas.\nSendo assim, pode-se perceber que hoje em todo momento de conexão com a internet temos um algoritmo nos observando, sendo necessário tanto para melhorar nossa produtividade, quanto para ajudar a encontrar a música que a gente tanto gosta de uma maneira mais fácil. Para as empresas, cria-se a necessidade de desenvolvimento nesse setor, já que o investimento retorna como lucro nas vendas."
  },
  {
    "objectID": "artigos/teste-hipoteses/index.html",
    "href": "artigos/teste-hipoteses/index.html",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "",
    "text": "“Uma moeda foi lançada 200 vezes e foram obtidas 127 coroas. Suspeita-se que a moeda seja desonesta para coroa, ou seja, o resultado coroa tem maior probabilidade de ocorrer. Verifique a desonestidade da moeda com no máximo 5% de chance de a conclusão ser errada.”\nExercícios como este são comuns quando queremos testar uma hipótese. Mesmo para aqueles que já têm familiaridade com o assunto, surgem várias perguntas ao olhar para o exemplo: “Por onde eu começo?” “Qual é a minha hipótese inicial?” “O que ele quer saber?” “Qual teste devo usar?”, entre outras. Este texto tem como objetivo servir de guia para simplificar a identificação e interpretação dessas questões, ajudando a saber exatamente o que fazer em cada situação."
  },
  {
    "objectID": "artigos/teste-hipoteses/index.html#definição-de-hipótese-nula-e-alternativa",
    "href": "artigos/teste-hipoteses/index.html#definição-de-hipótese-nula-e-alternativa",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "Definição de hipótese nula e alternativa",
    "text": "Definição de hipótese nula e alternativa\nO primeiro passo em um teste de hipótese é definir as hipóteses nula e alternativa. Uma hipótese nula representada por \\(H_0\\) é a hipótese inicial ou a suposição padrão sobre uma situação. No caso da moeda mencionada anteriormente, a primeira ideia ao estar em contato com uma moeda é esperar que ela seja honesta. Portanto, neste caso, \\(H_0 : P(c) = 0.5\\) em que \\(P(c)\\) representa a probabilidade de se obter coroa.\nA hipótese alternativa representada por \\(H_1\\) é aquilo que se suspeita ou deseja testar. No exemplo da moeda, suspeitamos que a moeda seja desonesta favorecendo o resultado coroa. Assim, a hipótese alternativa é formulada como \\(H_1 : P(c) &gt; 0.5\\)."
  },
  {
    "objectID": "artigos/teste-hipoteses/index.html#escolha-do-teste-estatístico",
    "href": "artigos/teste-hipoteses/index.html#escolha-do-teste-estatístico",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "Escolha do teste estatístico",
    "text": "Escolha do teste estatístico\nO segundo passo é escolher o teste estatístico adequado. Mario F. Triola em seu livro Introdução à Estatística disponibiliza uma tabela mostrando qual tipo de teste usar em cada caso:\n\n\n\n\n\n\n\n\n\nParâmetro\nDistribuição Amostral\nRequisitos\nEstatística de Teste\n\n\n\n\nProporção\nNormal (Z)\nnp ≥ 5 e nq ≥ 5\n\\[\\begin{equation}z = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} \\end{equation}\\]\n\n\nMédia (σ desconhecido)\nt-student\nPopulação normalmente distribuída ou n &gt; 30\n\\[\\begin{equation} t = \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\end{equation}\\]\n\n\nMédia (σ conhecido)\nNormal (Z)\nPopulação normalmente distribuída ou n &gt; 30\n\\[\\begin{equation} z = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\end{equation}\\]\n\n\nDesvio-padrão σ ou variância σ²\nχ²\nPopulação normalmente distribuída\n\\[\\begin{equation} \\chi^2 = \\frac{(n - 1)s^2}{\\sigma^2} \\end{equation}\\]\n\n\n\nNo exemplo da moeda, queremos estudar a proporção de coroas obtidas nos lançamentos. Portanto, o teste apropriado a ser utilizado é o teste z para proporções. Para garantir que este teste seja válido é essencial verificar se os requisitos do teste são atendidos.\nPara um teste z de proporções precisamos assegurar que tanto \\(np\\) quanto \\(nq\\) sejam maiores ou iguais a 5:\n\n\\(np\\) representa o número esperado de sucessos em que \\(n\\) é o tamanho da amostra e \\(p\\) é a proporção esperada de sucessos (em nosso caso, a proporção esperada de coroas);\n\\(nq\\) representa o número esperado de fracassos em que \\(q = 1 − p\\).\n\nNo caso da moeda assumindo que ela é honesta a probabilidade de obter coroa é 0.5 e a probabilidade de obter cara é também 0.5.\nDado que a moeda foi lançada 200 vezes \\((n=200)\\):\n\\[\\begin{equation}\nnp = nq = 200 \\times 0.5 = 100.\n\\end{equation}\\] Portanto, as condições \\(np ≥ 5\\) e \\(nq ≥ 5\\) são cumpridas. Isso confirma que podemos usar o teste z para proporções de forma válida neste caso.\n\\[\\begin{equation}\nz = \\frac{127/200 - 0.5}{\\sqrt{0.5 \\times (1-0.5) / 200}} \\approx 3.81\n\\end{equation}\\]"
  },
  {
    "objectID": "artigos/teste-hipoteses/index.html#determinação-do-valor-crítico",
    "href": "artigos/teste-hipoteses/index.html#determinação-do-valor-crítico",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "Determinação do valor crítico",
    "text": "Determinação do valor crítico\nO valor crítico é um ponto de corte que nos ajuda a decidir se devemos rejeitar a hipótese nula. Ele é determinado pelo nível de significância (\\(\\alpha\\)) do teste, que é a probabilidade máxima de cometer um erro do tipo I (rejeitar uma hipótese nula verdadeira).\nPara um nível de significância de 5% (\\(\\alpha = 0.05\\)), usamos tabelas de distribuição para encontrar o valor crítico correspondente. Existem dois tipos principais de testes estatísticos:\n\nTestes Unilaterais\nUm teste unilateral é usado quando a hipótese alternativa é direcionada, ou seja, espera-se que a estatística de teste seja significativamente maior ou menor que o valor da hipótese nula.\n\nTeste Unilateral à Direita: Usado quando a hipótese alternativa sugere que o parâmetro é maior que o valor da hipótese nula \\[\\begin{equation}H_1: \\mu &gt; \\mu_0\\end{equation}\\]\nTeste Unilateral à Esquerda: Usado quando a hipótese alternativa sugere que o parâmetro é menor que o valor da hipótese nula \\[\\begin{equation}H_1: \\mu &lt; \\mu_0\\end{equation}\\]\n\n\n\nTestes Bilaterais\nUm teste bilateral é usado quando a hipótese alternativa não é direcionada, ou seja, espera-se que a estatística de teste seja significativamente diferente (maior ou menor) do valor da hipótese nula.\n\nTeste Bilateral: Usado quando a hipótese alternativa sugere que o parâmetro é diferente do valor da hipótese nula \\[\\begin{equation}H_1: \\mu \\neq \\mu_0\\end{equation}\\]\n\nNo exemplo da moeda usaremos um teste unilateral à direita, já que a hipótese alternativa é \\(P(c) &gt; 0.5\\).\n\nO valor crítico da distribuição normal padrão é aproximadamente 1.645."
  },
  {
    "objectID": "artigos/teste-hipoteses/index.html#tomada-de-decisão",
    "href": "artigos/teste-hipoteses/index.html#tomada-de-decisão",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "Tomada de decisão",
    "text": "Tomada de decisão\nSe o valor resultante do teste for maior que o valor crítico rejeitamos a hipótese nula. No nosso caso como 3.81 &gt; 1.645 rejeitamos a hipótese de que a moeda é justa. Portanto concluímos com um nível de significância de 5% que a moeda não é honesta."
  },
  {
    "objectID": "artigos/basquete/index.html",
    "href": "artigos/basquete/index.html",
    "title": "Estatística no Basquete: Transformando a Escolha do MVP",
    "section": "",
    "text": "Atualmente, é cada vez mais perceptível que o uso da estatística no esporte resulta em melhorias tanto nas performances individuais quanto coletivas. Um exemplo disso é o aumento da procura de times e atletas por analistas, que, por meio da análise de dados, podem identificar padrões de desempenho de suas equipes e dos adversários, ajustar suas táticas de acordo com as tendências observadas e maximizar suas chances de vitória. Neste contexto, destacarei a importância desse uso, seu impacto e um aspecto que nem sempre é percebido: sua influência nas premiações individuais.\nMeu interesse por esse tema foi despertado recentemente ao ver um comentário de Shaquille O’Neal, jogador aposentado e atualmente comentarista, criticando a escolha de Nikola Jokic, o jogador em análise hoje, como ganhador do prêmio de MVP. Esse comentário me fez refletir sobre como o prêmio de MVP se transformou e como tem se tornado cada vez mais dependente de estatísticas abrangentes na escolha do vencedor."
  },
  {
    "objectID": "artigos/basquete/index.html#introdução",
    "href": "artigos/basquete/index.html#introdução",
    "title": "Estatística no Basquete: Transformando a Escolha do MVP",
    "section": "",
    "text": "Atualmente, é cada vez mais perceptível que o uso da estatística no esporte resulta em melhorias tanto nas performances individuais quanto coletivas. Um exemplo disso é o aumento da procura de times e atletas por analistas, que, por meio da análise de dados, podem identificar padrões de desempenho de suas equipes e dos adversários, ajustar suas táticas de acordo com as tendências observadas e maximizar suas chances de vitória. Neste contexto, destacarei a importância desse uso, seu impacto e um aspecto que nem sempre é percebido: sua influência nas premiações individuais.\nMeu interesse por esse tema foi despertado recentemente ao ver um comentário de Shaquille O’Neal, jogador aposentado e atualmente comentarista, criticando a escolha de Nikola Jokic, o jogador em análise hoje, como ganhador do prêmio de MVP. Esse comentário me fez refletir sobre como o prêmio de MVP se transformou e como tem se tornado cada vez mais dependente de estatísticas abrangentes na escolha do vencedor."
  },
  {
    "objectID": "artigos/basquete/index.html#mvp-o-que-é-e-como-funciona",
    "href": "artigos/basquete/index.html#mvp-o-que-é-e-como-funciona",
    "title": "Estatística no Basquete: Transformando a Escolha do MVP",
    "section": "MVP: O que é e Como Funciona",
    "text": "MVP: O que é e Como Funciona\nO prêmio de MVP, ou Most Valuable Player, é concedido anualmente ao jogador considerado mais impactante para sua equipe durante a temporada regular da NBA. A escolha do MVP é feita por um painel de jornalistas e especialistas em basquete, que votam no jogador que consideram mais qualificado para o prêmio com base em uma série de critérios, incluindo estatísticas individuais, contribuições para o sucesso da equipe, liderança e impacto geral no jogo. Anteriormente, a escolha do MVP era baseada principalmente em estatísticas básicas, mas agora inclui diversas métricas avançadas."
  },
  {
    "objectID": "artigos/basquete/index.html#exemplo-de-algumas-dessas-métricas",
    "href": "artigos/basquete/index.html#exemplo-de-algumas-dessas-métricas",
    "title": "Estatística no Basquete: Transformando a Escolha do MVP",
    "section": "Exemplo de Algumas Dessas Métricas:",
    "text": "Exemplo de Algumas Dessas Métricas:\nPlayer Efficiency Rating (PER): busca quantificar o impacto geral de um jogador em todas as áreas do jogo, considerando uma variedade de estatísticas individuais e ponderando-as de acordo com sua importância relativa.\nWin Shares (WS): calcula a contribuição de um jogador para as vitórias de sua equipe ao longo da temporada, levando em conta sua contribuição ofensiva, defensiva e sua presença em quadra.\nPts/jogo: representa a média de pontos que um jogador marca por jogo.\nReb/jogo: representa a média de rebotes que um jogador consegue por jogo.\nAst/jogo: representa a média de assistências que um jogador faz por jogo."
  },
  {
    "objectID": "artigos/basquete/index.html#análise-gráfica-dos-finalistas-do-prêmio-mvp",
    "href": "artigos/basquete/index.html#análise-gráfica-dos-finalistas-do-prêmio-mvp",
    "title": "Estatística no Basquete: Transformando a Escolha do MVP",
    "section": "Análise Gráfica dos Finalistas do Prêmio MVP",
    "text": "Análise Gráfica dos Finalistas do Prêmio MVP\nPara ilustrar a mudança no prêmio MVP devido ao aumento do peso das estatísticas, apresento uma análise gráfica com as principais estatísticas dos finalistas deste ano: Nikola Jokic, Shai Gilgeous-Alexander e Luka Doncic. Os dados utilizados para construir e analisar esse gráfico foram obtidos do site basketball-reference e correspondem à temporada regular 2023-24 dos três jogadores. A análise visa contrapor a declaração de Shaq: “Quero parabenizá-lo, mas quero que você ouça isso de mim primeiro. Achei que o SGA deveria ter sido o MVP, isso não é desrespeito a você, mas parabéns.”. Essa opinião parece ser baseada em uma visão tradicional da premiação, que favorece os jogadores mais dominantes.\n\n\n\n\n\n\n\n\n\nÉ possível observar no gráfico que, do ponto de vista estatístico, a escolha de Jokic como MVP não é um erro. Em todas as métricas analisadas, a diferença entre ele e os outros concorrentes não é significativa, e ele lidera em métricas como PER e WS, que melhor refletem o impacto de um jogador em sua equipe."
  },
  {
    "objectID": "artigos/basquete/index.html#conclusão",
    "href": "artigos/basquete/index.html#conclusão",
    "title": "Estatística no Basquete: Transformando a Escolha do MVP",
    "section": "Conclusão",
    "text": "Conclusão\nQuis destacar como a estatística tem influenciado positivamente o basquete, não apenas melhorando o desempenho individual e coletivo, mas também reconhecendo jogadores com habilidades anteriormente subestimadas, o que contribui para o desempenho da equipe. Além disso, em minha opinião, o uso da estatística aumenta a transparência e a objetividade na seleção do MVP. Ao considerar uma gama mais ampla de métricas, os eleitores do MVP podem tomar decisões mais informadas e justas, reconhecendo verdadeiramente o jogador mais valioso para sua equipe e para a liga como um todo."
  },
  {
    "objectID": "artigos_fila/teste-hipoteses/index.html",
    "href": "artigos_fila/teste-hipoteses/index.html",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "",
    "text": "“Uma moeda foi lançada 200 vezes e foram obtidas 127 coroas. Suspeita-se que a moeda seja desonesta para coroa, ou seja, o resultado coroa tem maior probabilidade de ocorrer. Verifique a desonestidade da moeda com no máximo 5% de chance de a conclusão ser errada.”\nExercícios como este são comuns quando queremos testar uma hipótese. Mesmo para aqueles que já têm familiaridade com o assunto, surgem várias perguntas ao olhar para o exemplo: “Por onde eu começo?” “Qual é a minha hipótese inicial?” “O que ele quer saber?” “Qual teste devo usar?”, entre outras. Este texto tem como objetivo servir de guia para simplificar a identificação e interpretação dessas questões, ajudando a saber exatamente o que fazer em cada situação."
  },
  {
    "objectID": "artigos_fila/teste-hipoteses/index.html#definição-de-hipótese-nula-e-alternativa",
    "href": "artigos_fila/teste-hipoteses/index.html#definição-de-hipótese-nula-e-alternativa",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "Definição de hipótese nula e alternativa",
    "text": "Definição de hipótese nula e alternativa\nO primeiro passo em um teste de hipótese é definir as hipóteses nula e alternativa. Uma hipótese nula representada por \\(H_0\\) é a hipótese inicial ou a suposição padrão sobre uma situação. No caso da moeda mencionada anteriormente, a primeira ideia ao estar em contato com uma moeda é esperar que ela seja honesta. Portanto, neste caso, \\(H_0 : P(c) = 0.5\\) em que \\(P(c)\\) representa a probabilidade de se obter coroa.\nA hipótese alternativa representada por \\(H_1\\) é aquilo que se suspeita ou deseja testar. No exemplo da moeda, suspeitamos que a moeda seja desonesta favorecendo o resultado coroa. Assim, a hipótese alternativa é formulada como \\(H_1 : P(c) &gt; 0.5\\)."
  },
  {
    "objectID": "artigos_fila/teste-hipoteses/index.html#escolha-do-teste-estatístico",
    "href": "artigos_fila/teste-hipoteses/index.html#escolha-do-teste-estatístico",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "Escolha do teste estatístico",
    "text": "Escolha do teste estatístico\nO segundo passo é escolher o teste estatístico adequado. Mario F. Triola em seu livro Introdução à Estatística disponibiliza uma tabela mostrando qual tipo de teste usar em cada caso:\n\n\n\n\n\n\n\n\n\nParâmetro\nDistribuição Amostral\nRequisitos\nEstatística de Teste\n\n\n\n\nProporção\nNormal (Z)\nnp ≥ 5 e nq ≥ 5\n\\[\\begin{equation}z = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} \\end{equation}\\]\n\n\nMédia (σ desconhecido)\nt-student\nPopulação normalmente distribuída ou n &gt; 30\n\\[\\begin{equation} t = \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}} \\end{equation}\\]\n\n\nMédia (σ conhecido)\nNormal (Z)\nPopulação normalmente distribuída ou n &gt; 30\n\\[\\begin{equation} z = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\end{equation}\\]\n\n\nDesvio-padrão σ ou variância σ²\nχ²\nPopulação normalmente distribuída\n\\[\\begin{equation} \\chi^2 = \\frac{(n - 1)s^2}{\\sigma^2} \\end{equation}\\]\n\n\n\nNo exemplo da moeda, queremos estudar a proporção de coroas obtidas nos lançamentos. Portanto, o teste apropriado a ser utilizado é o teste z para proporções. Para garantir que este teste seja válido é essencial verificar se os requisitos do teste são atendidos.\nPara um teste z de proporções precisamos assegurar que tanto \\(np\\) quanto \\(nq\\) sejam maiores ou iguais a 5:\n\n\\(np\\) representa o número esperado de sucessos em que \\(n\\) é o tamanho da amostra e \\(p\\) é a proporção esperada de sucessos (em nosso caso, a proporção esperada de coroas);\n\\(nq\\) representa o número esperado de fracassos em que \\(q = 1 − p\\).\n\nNo caso da moeda assumindo que ela é honesta a probabilidade de obter coroa é 0.5 e a probabilidade de obter cara é também 0.5.\nDado que a moeda foi lançada 200 vezes \\((n=200)\\):\n\\[\\begin{equation}\nnp = nq = 200 \\times 0.5 = 100.\n\\end{equation}\\] Portanto, as condições \\(np ≥ 5\\) e \\(nq ≥ 5\\) são cumpridas. Isso confirma que podemos usar o teste z para proporções de forma válida neste caso.\n\\[\\begin{equation}\nz = \\frac{127/200 - 0.5}{\\sqrt{0.5 \\times (1-0.5) / 200}} \\approx 3.81\n\\end{equation}\\]"
  },
  {
    "objectID": "artigos_fila/teste-hipoteses/index.html#determinação-do-valor-crítico",
    "href": "artigos_fila/teste-hipoteses/index.html#determinação-do-valor-crítico",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "Determinação do valor crítico",
    "text": "Determinação do valor crítico\nO valor crítico é um ponto de corte que nos ajuda a decidir se devemos rejeitar a hipótese nula. Ele é determinado pelo nível de significância (\\(\\alpha\\)) do teste, que é a probabilidade máxima de cometer um erro do tipo I (rejeitar uma hipótese nula verdadeira).\nPara um nível de significância de 5% (\\(\\alpha = 0.05\\)), usamos tabelas de distribuição para encontrar o valor crítico correspondente. Existem dois tipos principais de testes estatísticos:\n\nTestes Unilaterais\nUm teste unilateral é usado quando a hipótese alternativa é direcionada, ou seja, espera-se que a estatística de teste seja significativamente maior ou menor que o valor da hipótese nula.\n\nTeste Unilateral à Direita: Usado quando a hipótese alternativa sugere que o parâmetro é maior que o valor da hipótese nula \\[\\begin{equation}H_1: \\mu &gt; \\mu_0\\end{equation}\\]\nTeste Unilateral à Esquerda: Usado quando a hipótese alternativa sugere que o parâmetro é menor que o valor da hipótese nula \\[\\begin{equation}H_1: \\mu &lt; \\mu_0\\end{equation}\\]\n\n\n\nTestes Bilaterais\nUm teste bilateral é usado quando a hipótese alternativa não é direcionada, ou seja, espera-se que a estatística de teste seja significativamente diferente (maior ou menor) do valor da hipótese nula.\n\nTeste Bilateral: Usado quando a hipótese alternativa sugere que o parâmetro é diferente do valor da hipótese nula \\[\\begin{equation}H_1: \\mu \\neq \\mu_0\\end{equation}\\]\n\nNo exemplo da moeda usaremos um teste unilateral à direita, já que a hipótese alternativa é \\(P(c) &gt; 0.5\\).\n\nO valor crítico da distribuição normal padrão é aproximadamente 1.645."
  },
  {
    "objectID": "artigos_fila/teste-hipoteses/index.html#tomada-de-decisão",
    "href": "artigos_fila/teste-hipoteses/index.html#tomada-de-decisão",
    "title": "Teste de Hipóteses: Guia Prático para Análise Estatística",
    "section": "Tomada de decisão",
    "text": "Tomada de decisão\nSe o valor resultante do teste for maior que o valor crítico rejeitamos a hipótese nula. No nosso caso como 3.81 &gt; 1.645 rejeitamos a hipótese de que a moeda é justa. Portanto concluímos com um nível de significância de 5% que a moeda não é honesta."
  },
  {
    "objectID": "artigos/knn/index.html",
    "href": "artigos/knn/index.html",
    "title": "Por trás do modelo KNN",
    "section": "",
    "text": "O modelo dos K vizinhos mais próximos, do inglês K-Nearest Neighbors (KNN), é um algoritmo amplamente reconhecido por sua versatilidade e simplicidade. Ele trabalha com a ideia de proximidade no espaço das características. A premissa fundamental do KNN é que dados semelhantes tendem a estar localizados próximos uns dos outros. Ao considerar os K vizinhos mais próximos, o algoritmo usa essas informações para fazer previsões ou classificações sobre novos dados.\nAplicações do modelo KNN\nO modelo KNN possui diversas aplicações práticas. Na classificação de textos e e-mails, ele identifica spam ao comparar mensagens com aquelas previamente rotuladas. Em reconhecimento de imagens, facilita a identificação de objetos e padrões ao comparar características visuais com um banco de dados conhecido. No campo da saúde, o KNN auxilia no diagnóstico de doenças analisando sintomas e históricos de pacientes semelhantes. Em sistemas de recomendação, como os da Netflix e Spotify, o algoritmo sugere produtos com base nas preferências de usuários similares. No setor financeiro, o KNN é utilizado para avaliar o risco de crédito, comparando perfis financeiros para prever a probabilidade de inadimplência.\nComo é a implementação do KNN?\nO primeiro passo é definir o número de vizinhos, representado por “K” no algoritmo KNN. Em seguida, o algoritmo calcula as distâncias entre a nova observação e todas as observações do conjunto de dados. Após o cálculo das distâncias, o próximo passo é selecionar os K vizinhos mais próximos. Apesar da distância euclidianda ser a mais comum, também existem outras muito utilizadas como: Distância de Manhattan, Distância de Minkowski, Distância Cosseno, entre outras.\nSuponha que temos um novo dado, representado pelo círculo cinza, que queremos classificar como Classe A (círculo laranja) ou Classe B (círculo azul). As observações existentes foram plotadas de acordo com as variáveis X e Y, conforme gráfico abaixo:\n\n\n\n\n\nNo primeiro cenário, com K=3, após calcular todas as distâncias euclidianas entre o ponto cinza e os demais pontos, a seleção dos vizinhos mais próximos inclui um ponto laranja e dois pontos azuis. Com base nessa seleção, o algoritmo classifica o ponto cinza como azul (Classe B). No entanto, ao ajustar K para 6, a seleção passa a abranger dois pontos azuis e quatro pontos laranjas, resultando na classificação do ponto cinza como laranja (Classe A). Então, como posso saber qual é o melhor K?\nPré-processamento e Validação dos Dados\nAntes de aplicar o algoritmo KNN, é essencial realizar um pré-processamento adequado dos dados para garantir resultados precisos e confiáveis. Esse processo geralmente envolve a normalização ou padronização dos dados, uma vez que as características podem estar em escalas diferentes. Por exemplo, uma variável pode ser medida em milímetros, enquanto outra é expressa em quilogramas. Para evitar distorções nos resultados, é necessário ajustar essas escalas, assegurando que todas as características contribuam de maneira equitativa no cálculo das distâncias.\nAlém do pré-processamento, a validação dos dados é crucial para avaliar a performance do modelo de forma precisa. A validação cruzada é uma técnica eficaz que divide o conjunto de dados em várias partes. O modelo é treinado em uma parte dos dados e testado nas partes restantes, repetindo esse processo para todas as combinações possíveis de treinamento e teste. Esse método oferece uma avaliação mais robusta e ajuda a evitar o overfitting, que ocorre quando o modelo se ajusta excessivamente aos dados de treinamento e falha em generalizar para novos dados. Ajustar o valor de K e aplicar técnicas de validação apropriadas pode melhorar significativamente a performance e a confiabilidade das previsões.\nConsiderações sobre o modelo\nApesar de ser muito eficiente e prático, o custo computacional pode se tornar elevado quando lidamos com grandes volumes de dados, devido à necessidade de calcular múltiplas distâncias. A escolha do valor de K também é crucial: um K muito pequeno pode resultar em sensibilidade excessiva a ruídos, enquanto um K muito grande pode suavizar detalhes importantes e comprometer a precisão. Além disso, o KNN tende a funcionar melhor com um número reduzido de dimensões, pois quando enfrentamos muitos atributos, a performance pode ser prejudicada.\nConclusão\nO KNN se destaca como uma ferramenta poderosa e versátil para classificação e regressão, oferecendo uma abordagem intuitiva e eficaz para análise de dados. Sua simplicidade na implementação e a capacidade de lidar com dados variados tornam o KNN uma escolha popular em muitas áreas. No entanto, para garantir que o modelo forneça resultados precisos e confiáveis, é fundamental realizar um pré-processamento adequado dos dados, como normalização e padronização, e aplicar técnicas robustas de validação."
  },
  {
    "objectID": "artigos/bruno/index.html",
    "href": "artigos/bruno/index.html",
    "title": "Aumente suas chances de não ser roubado em Uberlândia",
    "section": "",
    "text": "Introdução\nCaro uberlandense, cansado de ter seus pertences roubados enquanto anda tranquilamente pela rua? Agoniado por não saber quando e onde pode ser vítima de um possível roubo? Não se desespere (ainda)! Vamos analisar juntos algumas informações referentes a furtos e roubos que ocorreram na cidade de Uberlândia durante todo o ano de 2023, buscando possíveis padrões que possam nos ajudar a identificar o comportamento destes crimes e assim aumentarmos nossas chances de não sermos a próxima vítima. Os dados analisados neste texto foram obtidos através do site da Secretaria de Estado de Justiça e Segurança Pública de Minas Gerais (SEJUSP-MG).\n\n\nEm que horário é mais seguro andar pela cidade?\nVamos começar pela parte mais importante: o horário. Será que esses crimes acontecem mais durante a manhã, tarde ou noite? Analisaremos faixas de 6 horas, conforme o gráfico abaixo:\n\n\n\n\n\n\n\n\n\n(Bom, para aqueles que saem muito a partir das 18:00, sugiro que não o façam)\nEste comportamento seria o esperado visto que é um horário com grande movimento de pessoas, porém, diferentemente dos horários entre 06:00 e 17:59, a partir das 18:00 a baixa iluminação pode aumentar a confiança dos assaltantes. Portanto, aumentar os cuidados durante a noite é fundamental.\nEmbora a descoberta anterior possa parecer um pouco “óbvia” demais, é importante entender alguns dos motivos que fazem isso acontecer, por exemplo, o movimento de pessoas durante a madrugada (00:00 às 05:59) é baixo, porém, esta faixa de horário apresenta a segunda maior taxa de ocorrências, o que pode contribuir para a hipótese de que a baixa luminosidade contribua para a ocorrência destes roubos\n\n\nQual dia da semana é o mais seguro?\nAgora que avaliamos os horários que esses crimes ocorrem, vamos buscar entender como esses roubos estão dispostos no decorrer da semana. Para isso, vamos analisar a quantidade de roubos por dia da semana a partir do gráfico a seguir:\n\n\n\n\n\n\n\n\n\n(Podemos perceber que até os assaltantes preferem “trabalhar” em dias úteis)\nTemos agora uma informação um pouco contraintuitiva (ou não): se o número de roubos acontece, em sua maioria, durante a noite, era de se esperar que essa quantidade aumentasse aos finais de semana onde o fluxo de pessoas neste horário é ainda maior. Entretanto, não é isso que o gráfico anterior nos diz.\n\n\nConclusão\nPercebe-se que, com apenas uma análise rápida e simplória, podemos entender um pouco mais o comportamento dos roubos na cidade de Uberlândia, como estão distribuídos durante a semana e seus principais horários de ocorrência. Um estudo mais detalhado poderia chegar a resultados ainda mais interessantes.\nE lembre-se, muito cuidado ao sair de casa quinta-feira à noite."
  },
  {
    "objectID": "artigos/roubos-uberlandia/index.html",
    "href": "artigos/roubos-uberlandia/index.html",
    "title": "Aumente suas chances de não ser roubado em Uberlândia",
    "section": "",
    "text": "Introdução\nCaro uberlandense, cansado de ter seus pertences roubados enquanto anda tranquilamente pela rua? Agoniado por não saber quando e onde pode ser vítima de um possível roubo? Não se desespere (ainda)! Vamos analisar juntos algumas informações referentes a furtos e roubos que ocorreram na cidade de Uberlândia durante todo o ano de 2023, buscando possíveis padrões que possam nos ajudar a identificar o comportamento destes crimes e assim aumentarmos nossas chances de não sermos a próxima vítima. Os dados analisados neste texto foram obtidos através do site da Secretaria de Estado de Justiça e Segurança Pública de Minas Gerais (SEJUSP-MG).\n\n\nEm que horário é mais seguro andar pela cidade?\nVamos começar pela parte mais importante: o horário. Será que esses crimes acontecem mais durante a manhã, tarde ou noite? Analisaremos faixas de 6 horas, conforme o gráfico abaixo:\n\n\n\n\n\n\n\n\n\n(Bom, para aqueles que saem muito a partir das 18:00, sugiro que não o façam)\nEste comportamento seria o esperado visto que é um horário com grande movimento de pessoas, porém, diferentemente dos horários entre 06:00 e 17:59, a partir das 18:00 a baixa iluminação pode aumentar a confiança dos assaltantes. Portanto, aumentar os cuidados durante a noite é fundamental.\nEmbora a descoberta anterior possa parecer um pouco “óbvia” demais, é importante entender alguns dos motivos que fazem isso acontecer, por exemplo, o movimento de pessoas durante a madrugada (00:00 às 05:59) é baixo, porém, esta faixa de horário apresenta a segunda maior taxa de ocorrências, o que pode contribuir para a hipótese de que a baixa luminosidade contribua para a ocorrência destes roubos\n\n\nQual dia da semana é o mais seguro?\nAgora que avaliamos os horários que esses crimes ocorrem, vamos buscar entender como esses roubos estão dispostos no decorrer da semana. Para isso, vamos analisar a quantidade de roubos por dia da semana a partir do gráfico a seguir:\n\n\n\n\n\n\n\n\n\n(Podemos perceber que até os assaltantes preferem “trabalhar” em dias úteis)\nTemos agora uma informação um pouco contraintuitiva (ou não): se o número de roubos acontece, em sua maioria, durante a noite, era de se esperar que essa quantidade aumentasse aos finais de semana onde o fluxo de pessoas neste horário é ainda maior. Entretanto, não é isso que o gráfico anterior nos diz.\n\n\nConclusão\nPercebe-se que, com apenas uma análise rápida e simplória, podemos entender um pouco mais o comportamento dos roubos na cidade de Uberlândia, como estão distribuídos durante a semana e seus principais horários de ocorrência. Um estudo mais detalhado poderia chegar a resultados ainda mais interessantes.\nE lembre-se, muito cuidado ao sair de casa quinta-feira à noite."
  },
  {
    "objectID": "atividades.html",
    "href": "atividades.html",
    "title": "Mapa do tesouro",
    "section": "",
    "text": "Nesta curso, o PET Estatística apresenta o conteúdo de Raspagem de Dados (Web scraping) e Construção de Mapas com R. O curso será dividido em duas aulas.\n\n\n\nMapa representando o número de queimadas registradas em 2024 por estado no Brasil; quanto mais forte a cor, maior o número de queimadas registradas."
  },
  {
    "objectID": "atividades.html#informações-gerais-sobre-o-curso",
    "href": "atividades.html#informações-gerais-sobre-o-curso",
    "title": "Mapa do tesouro",
    "section": "",
    "text": "Nesta curso, o PET Estatística apresenta o conteúdo de Raspagem de Dados (Web scraping) e Construção de Mapas com R. O curso será dividido em duas aulas.\n\n\n\nMapa representando o número de queimadas registradas em 2024 por estado no Brasil; quanto mais forte a cor, maior o número de queimadas registradas."
  },
  {
    "objectID": "atividades.html#aula-1-raspagem-de-dados",
    "href": "atividades.html#aula-1-raspagem-de-dados",
    "title": "Mapa do tesouro",
    "section": "Aula 1: Raspagem de Dados",
    "text": "Aula 1: Raspagem de Dados\nNa primeira aula, introduziremos o conceito de raspagem de dados e apresentaremos a biblioteca rvest do R. A raspagem de dados é uma técnica utilizada para extrair informações de sites da internet. A biblioteca rvest é uma ferramenta poderosa para realizar essa tarefa. Também, como ferramenta eseencial para a raspagem de dados, apresentaremos os conceitos introdutórios de HTML e de expressões regulares.\nPara esta aula, rasparemos os dados de dois sites: um site sobre filmes e um site que apresenta dados sobre a alfabetização no Brasil. Os links para cada um desses sites estão disponíveis abaixo:\n\nSite sobre filmes\nSite sobre alfabetização no Brasil\n\nO código em R da primeira aula está aqui: Código da Aula 1"
  },
  {
    "objectID": "atividades.html#aula-2-construção-de-mapas",
    "href": "atividades.html#aula-2-construção-de-mapas",
    "title": "Mapa do tesouro",
    "section": "Aula 2: Construção de Mapas",
    "text": "Aula 2: Construção de Mapas\nNa segunda aula, apresentaremos os elementos necessários para construir mapas com o R. Utilizaremos os conceitos da primeira aula para obter dados e representar estes dados em mapas. Para esta aula, utilizaremos, por exemplo, os dados de queimadas registradas em 2024 por estado no Brasil. O mapa gerado com esses dados está disponível no início desta página.\nNo início da aula, ensinaremos os fundamentos do pacote ggplot2. Esta biblioteca poderosa nos permite criar gráficos muito bonitos. Os dados que utilizaremos estão disponíveis nos seguintes links:\n\nDados 1\nDados 2"
  },
  {
    "objectID": "atividades.html#bibliografia",
    "href": "atividades.html#bibliografia",
    "title": "Mapa do tesouro",
    "section": "Bibliografia",
    "text": "Bibliografia\nRecomendamos a leitura dos seguintes materiais para aprofundar o conhecimento sobre os temas abordados no curso:\n\nR para Ciência de Dados\nDocumentação da biblioteca geobr"
  },
  {
    "objectID": "programanas.html",
    "href": "programanas.html",
    "title": "Programanas",
    "section": "",
    "text": "A discriminação de gênero presente no Brasil tem impactado negativamente a relação de meninas e mulheres com a tecnologia desde cedo, o que se reflete em números alarmantes: apenas 17% das pessoas que programam no país são mulheres e somente 35% das pessoas que estudam áreas relacionadas a ciências, tecnologia, matemática e engenharia são do sexo feminino. Além disso, dados mostram que apenas 15% das matrículas em cursos de Ciências da Computação e Engenharias no Brasil são de mulheres. Essa realidade é também observada pelo grupo PET Estatística, que constatou a predominância de homens nos minicursos de programação que promove. Diante disso, o projeto Programanas surge como uma iniciativa essencial para promover a inclusão de meninas no universo da programação. Oferecendo cursos exclusivamente para mulheres, o projeto busca criar um ambiente acolhedor e inclusivo onde as integrantes podem trocar experiências e tirar dúvidas de programação.\n\n\nO objetivo do Programanas é promover a inclusão de meninas no universo da programação, oferecendo cursos exclusivamente para mulheres e criando um ambiente acolhedor e inclusivo. A iniciativa busca ainda fomentar a participação feminina na área de tecnologia, contrapondo- se ao cenário desfavorável atual, no qual há pouca representatividade das mulheres nesse setor. Através da troca de experiências e do suporte às dúvidas de programação, o projeto visa desenvolver habilidades e competências em mulheres para que possam ingressar e se destacar em áreas tecnológicas. Além disso, a iniciativa tem como objetivo estimular a diversidade de gênero no mercado de trabalho e em cursos de exatas, contribuindo para a construção de um ambiente profissional e acadêmico mais inclusivo e igualitário!\n\n\n\nNosso projeto está de cara nova. Nesta edição será ofertado um curso de Introdução à Ciência de Dados no R. O curso será ministrado por alunas do PET Estatística e abordará conceitos básicos de programação em R, manipulação de dados e visualização de dados.\nPara a primeira aula, estudaremos um conjunto de dados que contém informações sobre algumas espécies de pinguins. O conjunto palmerpenguins é um dos conjuntos de dados mais populares para quem está começando a aprender ciência de dados e é muito utilizado em cursos introdutórios de R. O conjunto de dados contém informações sobre a espécie, o sexo, o peso, o comprimento do bico e da asa, entre outras variáveis.\nPara acessar o conjunto, clique aqui."
  },
  {
    "objectID": "programanas.html#objetivos-do-projeto",
    "href": "programanas.html#objetivos-do-projeto",
    "title": "Programanas",
    "section": "",
    "text": "O objetivo do Programanas é promover a inclusão de meninas no universo da programação, oferecendo cursos exclusivamente para mulheres e criando um ambiente acolhedor e inclusivo. A iniciativa busca ainda fomentar a participação feminina na área de tecnologia, contrapondo- se ao cenário desfavorável atual, no qual há pouca representatividade das mulheres nesse setor. Através da troca de experiências e do suporte às dúvidas de programação, o projeto visa desenvolver habilidades e competências em mulheres para que possam ingressar e se destacar em áreas tecnológicas. Além disso, a iniciativa tem como objetivo estimular a diversidade de gênero no mercado de trabalho e em cursos de exatas, contribuindo para a construção de um ambiente profissional e acadêmico mais inclusivo e igualitário!"
  },
  {
    "objectID": "programanas.html#nova-edição-curso-introdutório-de-ciência-de-dados-no-r",
    "href": "programanas.html#nova-edição-curso-introdutório-de-ciência-de-dados-no-r",
    "title": "Programanas",
    "section": "",
    "text": "Nosso projeto está de cara nova. Nesta edição será ofertado um curso de Introdução à Ciência de Dados no R. O curso será ministrado por alunas do PET Estatística e abordará conceitos básicos de programação em R, manipulação de dados e visualização de dados.\nPara a primeira aula, estudaremos um conjunto de dados que contém informações sobre algumas espécies de pinguins. O conjunto palmerpenguins é um dos conjuntos de dados mais populares para quem está começando a aprender ciência de dados e é muito utilizado em cursos introdutórios de R. O conjunto de dados contém informações sobre a espécie, o sexo, o peso, o comprimento do bico e da asa, entre outras variáveis.\nPara acessar o conjunto, clique aqui."
  },
  {
    "objectID": "dashboard.html",
    "href": "dashboard.html",
    "title": "Dashboard",
    "section": "",
    "text": "O PET Estatística da Universidade Federal de Uberlândia criou um dashboard (painel dinâmico) sobre os crimes violentos em Minas Gerais. Para a construção desse dashbaord, utilizamos os dados disponibilizados no site da Secretaria de Justiça e Segurança Pública.\nNo painel é possível comparar as cidades do estado por categoria de crime, como roubo, homicídio, sequestro etc. Os registros são de 2012 até o presente. O dashboard está dividido em seções. A seção Estatísticas apresenta indicadores, gráficos e séries temporais para análise dos dados criminais. Já a seção Rankings exibe a posição das cidades pela taxa de criminalidade, permitindo comparar os diferentes municípios. Por fim, a seção Mapas exibe um mapa de calor da criminalidade no estado, facilitando a visualização da distribuição geográfica dos crimes.\nPara explorar o Painel Dinâmico, clique na imagem abaixo:"
  },
  {
    "objectID": "primeiros-passos.html",
    "href": "primeiros-passos.html",
    "title": "Primeiros passos",
    "section": "",
    "text": "Primeiros passos"
  },
  {
    "objectID": "artigos/blaze/index.html",
    "href": "artigos/blaze/index.html",
    "title": "A Blaze não manipula",
    "section": "",
    "text": "O site de apostas e cassino online Blaze não manipula seus jogos. Pelo menos, não o jogo Double. Essa é a tese que tentarei defender nesse texto. Isso significa que a empresa é honesta? Não. Nesse sentido não posso supor nada. De qualquer forma, sendo honesta ou não, manipulando ou não, pretendo deixar claro que apostar em cassino — mais especificamente no jogo Double — definitivamente não é uma fonte de renda extra.\n\nO que é a Blaze?\nA Blaze é uma plataforma de apostas que ficou muito conhecida no Brasil. Com um investimento massivo em marketing, que vai do patrocínio master do Santos Futebol Clube à publicidade nas redes de grandes influenciadores digitais como Neymar e Felipe Neto, a empresa conquistou uma imensa quantidade de apostadores e polêmicas. Apesar de sua atividade ser proibida no Brasil, a Blaze atua ilegalmente por meio da internet, com sede em Curaçao, uma pequena ilha no Caribe.\n\n\nO que é o jogo Double?\nO jogo funciona como uma espécie de roleta, na qual existem sete números vermelhos (de 1 a 7), sete números pretos (de 8 a 14) e um número branco (o zero). Para jogar, você deve apostar uma quantia x na cor que você acredita que será sorteada. Caso erre, você perde o valor apostado. Caso você aposte no preto ou no vermelho e acerte, você recebe 2x, duas vezes o valor que apostou. Caso você tenha acertado no branco, você recebe 14x.\n\nVamos usar um exemplo para que fique mais claro. Você, sensitivo, decide apostar 10 reais no vermelho, porque seu signo é Gêmeos, e geminianos são sanguíneos. Deu vermelho, é claro. Você recebe 20 reais — os 10 reais que você apostou mais 10 reais de lucro. Caso tivesse saído preto ou branco, você perderia a sua aposta. Acontece. Na próxima rodada é só acertar a cor e então você recuperará o que foi perdido e estará pronto para começar a lucrar. Nada mal.\n\n\nO jogo é manipulado?\nA resposta é simples: não sabemos. O que podemos fazer é analisar os dados fornecidos pelo site. Lá existe um histórico com mais de 800 páginas contendo 100 resultados cada. São mais de 80.000 resultados com a data e o horário em que foram sorteados. Há também um gráfico que mostra a proporção das últimas 3.000 rodadas, como na imagem abaixo. Pode-se argumentar que esses dados são falsos, mas isso é pouco provável.\n\n\n\nEntão o jogo é honesto?\nSe honesto, neste caso, significa que o jogo é realmente aleatório, ou seja, não há manipulação, é o que vamos verificar. Até o momento da produção deste texto, toda a explicação sobre o jogo é a que está descrita acima. Diante disso, vamos supor que está implícito que cada um dos quinze números (de 0 a 14) tem probabilidade igual de ser sorteado. Como são sete vermelhos, sete pretos e um branco, as probabilidades para cada uma das cores são:\nVermelho: 7/15 (46,66% aproximadamente)  Preto: 7/15 (46,66% aproximadamente)  Branco: 1/15 (6,66% aproximadamente) \nIsso é o que esperamos para um jogo realmente aleatório, e pelo gráfico podemos notar que a proporção está bem próxima disso. Então, nesse sentido, ponto a favor da honestidade.\nAlém disso, caso o jogo não fosse aleatório, no sentido de existir algum algoritmo para fraudar os resultados, o padrão do jogo poderia ser descoberto por meio de uma análise de dados. Poderíamos coletar os resultados do jogo por meio de Web Scraping e utilizar ferramentas como o R para analisá-los.\nNão vamos discutir aqui como um computador gera resultados aleatórios. Esse assunto ficará para um texto futuro. Não podemos negar também que um ou outro resultado específico possa ser adulterado. No entanto, os pontos levantados até aqui nos permitem dizer que é muito improvável que o Double seja manipulado.\nPor outro lado, se honesto significa que o jogador tem chance de se dar bem no jogo, então não. O jogo não é honesto. E basta que ele seja aleatório. Para provar isso, vamos usar o conceito de esperança matemática, ou valor esperado.\n\n\nO que é esperança?\nEsperança é o valor médio que se espera que uma variável aleatória assuma quando o experimento aleatório é repetido um número significativamente grande de vezes. A variável aleatória tem esse nome porque cada um dos valores que ela pode assumir está associado a uma probabilidade.\nExemplo: Vamos chamar de D a variável aleatória que representa a quantidade em dinheiro de uma aposta feita nesse jogo. Vamos supor que o jogador irá apostar 10 reais na cor vermelha. A variável D pode assumir os valores 10 (lucro de 10 reais), caso o resultado do experimento aleatório seja a cor vermelha; ou -10, caso o resultado seja ou preto ou branco.\nA fórmula da esperança é o somatório de cada valor da variável aleatória multiplicado pela sua probabilidade. Assim, se a variável \\(D\\) assume os valores \\(d_1\\), \\(d_2\\),…,\\(d_n\\) e estes valores acontecem com probabilidades \\(p(d_1) = P(D = d_1)\\), \\(p(d_2) = P(D = d_2)\\), …, \\(p(d_n) = P(D = d_n)\\), então a esperança de \\(D\\) será:\n\\[\nE[D] = \\sum_{i=1}^n d_ip(d_i)\n\\]\nUsando as probabilidades e os valores possíveis da variável \\(D\\), calculamos a sua esperança:\n\\[E[D] = 10 \\times (7/15) + (-10) x (7/15) + (-10) \\times (1/15) = -10/15.\\]\nLogo, \\(E[D]\\) é aproximadamente -0,67. Ou seja, para cada aposta de 10 reais na cor vermelha (ou na preta) espera-se que você perca, em média, 67 centavos. “E se eu jogar na cor branca?”, você pode estar pensando. Façamos as contas. A variável \\(D\\) agora pode assumir outros valores: -10, se sair vermelho, -10 se sair preto e 130 se sair branco (140 menos 10 da aposta):\n\\[E[D] = (-10) \\times (7/15) + (-10) \\times (7/15) + 130 \\times (1/15) = -10/15\\]\n\\(E[D]\\) é aproximadamente -0,67; o mesmo resultado.\n\n\nVamos simular na linguagem R\nPrimeiro vamos criar um vetor que representa 7 vermelhos, 7 pretos e 1 branco. Depois, vamos sortear um dos seus valores 3.000 vezes.\n\n# Cria a roleta do jogo Double\ndouble &lt;- c(rep('v', 7), rep('p', 7), 'b')\ndouble\n\n# Sorteia 3.000 vezes\nqtd_sorteio &lt;- 3000\nsorteio &lt;- sample(double, qtd_sorteio, replace = TRUE)\nsorteio\n\n# Calcula a proporção para cada cor\nmean(sorteio == 'v')\nmean(sorteio == 'p')\nmean(sorteio == 'b')\n\ncores &lt;- c('vermelho', 'preto', 'branco')\nproporcao &lt;- c(mean(sorteio == 'v'), mean(sorteio == 'p'), mean(sorteio == 'b'))\n\n# Cria uma tabela com as proporções\ntabela &lt;- data.frame(cores, proporcao)\n\nVamos criar um gráfico para as proporções obtidas nesta simulação:\n\n# Carrega o pacote ggplot.\nlibrary(ggplot2)\n\n## Gera o gráfico de barras para a proporção de cada cor\nggplot(tabela, aes(x=cores, y=proporcao, fill=cores)) +\n  coord_cartesian(ylim=c(0,1)) +\n  geom_col() +\n  scale_fill_manual(values = c(\"#FFFFFF\", \"#000000\", \"#FF0000\"), guide = \"none\") +\n  geom_text(aes(label=sprintf(\"%0.4f\", proporcao)), size = 5, \n            colour = c(\"#000000\", \"#000000\", \"#000000\"), nudge_y = 0.05) +\n  labs(x = NULL, y = NULL, title = \"Proporção das cores sorteadas\", colours = NULL) +\n  theme_light() +\n  theme(panel.background=element_rect(fill = \"#DCDCDC\"))\n\n\n\n\n\n\nPodemos notar que a proporção simulada é muito próxima da proporção calculada anteriormente e da proporção da Blaze. Agora vamos simular os resultados para três jogadores que apostam 10 reais, cada um em uma mesma cor, em todas as rodadas.\n\nvalor_aposta &lt;- 10\n\napostas_vermelho &lt;- c(rep('v', qtd_sorteio))\napostas_preto &lt;- c(rep('p', qtd_sorteio))\napostas_branco &lt;- c(rep('b', qtd_sorteio))\n\nresultados_vermelho &lt;- c()\nresultados_preto &lt;- c()\nresultados_branco &lt;- c()\n\nmontante_vermelho &lt;- 10\nmontante_preto &lt;- 10\nmontante_branco &lt;- 10\n\nhistorico_vermelho &lt;- c()\nhistorico_preto &lt;- c()\nhistorico_branco &lt;- c()\n\nfor(i in 1:length(sorteio)) {\n  if(sorteio[i] == 'v') {\n    resultados_vermelho[i] &lt;- +(valor_aposta)\n    resultados_preto[i] &lt;- -(valor_aposta)\n    resultados_branco[i] &lt;- -(valor_aposta)\n    \n    montante_vermelho &lt;- montante_vermelho + (valor_aposta)\n    montante_preto &lt;- montante_preto - (valor_aposta)\n    montante_branco &lt;- montante_branco - (valor_aposta)\n  } \n  else if (sorteio[i] == 'p') {\n    resultados_vermelho[i] &lt;- -(valor_aposta)\n    resultados_preto[i] &lt;- +(valor_aposta)\n    resultados_branco[i] &lt;- -(valor_aposta)\n    \n    montante_vermelho &lt;- montante_vermelho - (valor_aposta)\n    montante_preto &lt;- montante_preto + (valor_aposta)\n    montante_branco &lt;- montante_branco - (valor_aposta)\n  }\n  else if (sorteio[i] == 'b') {\n    resultados_vermelho[i] &lt;- -(valor_aposta)\n    resultados_preto[i] &lt;- -(valor_aposta)\n    resultados_branco[i] &lt;- +(13 * valor_aposta)\n    \n    montante_vermelho &lt;- montante_vermelho - (valor_aposta)\n    montante_preto &lt;- montante_preto - (valor_aposta)\n    montante_branco &lt;- montante_branco + (13 * valor_aposta)\n  }\n  historico_vermelho[i] &lt;- montante_vermelho\n  historico_preto[i] &lt;- montante_preto\n  historico_branco[i] &lt;- montante_branco\n}\n\nrodada &lt;- seq(1:qtd_sorteio)\n\ncria_dataframe &lt;- function(rod, hist, cor_nome) {\n  rodada &lt;- seq(1:rod)\n  historico &lt;- hist\n  cor &lt;- cor_nome\n  df &lt;- data.frame(rodada, historico, cor)\n  df\n}\n\ndf_vermelho &lt;- cria_dataframe(qtd_sorteio, historico_vermelho, 'vermelho')\n\ndf_preto &lt;- cria_dataframe(qtd_sorteio, historico_preto, 'preto')\n\ndf_branco &lt;-  cria_dataframe(qtd_sorteio, historico_branco, 'branco')\n\ndf_historico &lt;- rbind(df_vermelho, df_preto, df_branco)\n\nVamos agora criar um gráfico para analisar o saldo dos jogadores após as 3000 rodadas. A linha azul representa 0 reais. Acima dela, o jogador tem lucro; abaixo, prejuízo.\n\nggplot(df_historico, aes(x=rodada, y=historico, color=cor)) +\n  geom_line(linewidth = 1.5) +\n  scale_color_manual(values = c(\"#FFFFFF\", \"#000000\", \"#FF0000\"), guide=\"none\") +\n  coord_cartesian(xlim=c(0,max(rodada)+40)) +\n  geom_text(data = subset(df_historico, rodada == 3000), aes(label=historico, fontface=\"bold\"), \n            nudge_x = 85, size = 5) +\n  geom_hline(yintercept = 0, colour = \"blue\", linewidth = 1.5) +\n  labs(x=NULL, y=NULL, title=\"Saldo após cada rodada\", subtitle=\"Aposta de 10 reais\") +  \n  theme_light() +\n  theme(panel.background=element_rect(fill = \"gray\"))\n\n Em todos os três resultados, notamos que o resultado é prejuízo. Em alguns momentos o jogador que apostou no branco teve lucro, mas por poucas rodadas e depois de sofrer uma perda de mais de 2500 reais. Essa alta variabilidade da aposta na cor branca se deve ao conceito de variância, assunto que abordaremos em um artigo futuro.\nPor outro lado, para as três cores, a esperança — já calculada acima — nos mostra que em média espera-se um prejuízo de 0,67 para cada 10 reais de aposta. Portanto, em 3000 rodadas, o esperado é que o jogador perca 0,67 x 3000 = 2000 reais, resultado relativamente próximo do que foi simulado.\n\n\nConclusões\nÉ improvável que o jogo Double seja manipulado. No entanto, como provado pelos cálculos e pelas simulações, não vale a pena jogar. Sim, você pode ganhar duas, três, dez ou até cem vezes, o que pode gerar uma sensação positiva de que o jogo vale a pena. Mas só poderemos dizer que deu certo, ou deu lucro, quando você parar de jogar. Porém, caso esteja ganhando, por que você pararia de jogar? E, no longo prazo, o esperado é que você perca. Não tem como dar certo.\nDe qualquer forma, acredito que está provado que esse tipo de aposta não é, em nenhuma hipótese, uma fonte de renda extra. Agora, se você está pensando em colocar um dinheiro apenas para brincar, recomendamos que não faça isso. Os jogos de azar têm um caráter viciante e já são considerados um problema de saúde pública. Não aposte sua saúde financeira e mental."
  }
]